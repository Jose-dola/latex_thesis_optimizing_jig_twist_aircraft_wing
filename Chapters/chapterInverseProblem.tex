\chapter[Inverse problem]{Inverse problem: a novel framework for design optimization} \label{chapter_inverseProblem} 

An inverse problem is a problem where it is required to find the causal factors of some observed or desired outcomes of interest. 
Many problems in Engineering are inverse problems (see, e.g., \textcite{tanaka1998}; \textcite{dulikravich2000}; \textcite{dulikravich2001}; or \textcite{tanaka2003}). 
%Therefore, there has been a growing interest in finding techniques to tackle them from the Engineering community in the last decades (\textcite{neto2012}).
Therefore, the engineering has taken a growing interest in finding techniques to tackle them in the last decades (\textcite{neto2012}).

Metaheuristics are a set of methods capable to deal with some of those inverse problems.
Although they do not guarantee that an optimal solution is found, they may provide a sufficiently good solution (\textcite{blum2001}).
\textcite{yang2010} explains metaheuristic applications in Engineering and most of its algorithms: Simulated Annealing (SA), genetic algorithms, ant algorithms, bee algorithms, particle swarm optimization, etc. In addition, this reference gives information about random number generators, Monte Carlo methods, Markov chains, and other sampling methods and concepts.
This chapter provides a review of some of the aforementioned methods and concepts.

The main novelties of this thesis are given in sections \ref{sec_samplingWithVariablesConstraints} and \ref{sec_inverseDesign}.
An inverse problem is stated. 
The aim is to find a probability distribution in the input space of a surrogate model that satisfies a prescribed performance in the output when uncertainties are propagated.
%A novel framework is introduced to tackle this problem.
A novel framework that tackles this problem and has been developed by the author of this thesis is introduced in section \ref{sec_inverseDesign}.
%A novel framework developed by the author of this thesis that tackles this problem is introduced in section \ref{sec_inverseDesign}.
%A novel framework that tackles this problem is introduced.
%The main goal of this Chapter is to introduce a novel framework for design optimization (section \ref{sec_inverseDesign}). Sampling from a distribution is a fundamental process for this method, and therefore the first part of this Chapter will be dedicated to that (section \ref{sec_sampling}).

\section{Sampling from a probability distribution} \label{sec_sampling}

This section gives a review of different methods to sample from a PDF and explains how computers implement these methods.

\subsection{Pseudorandom number generators} \label{sec_pseudorandomNumberGenerator}

The problem of sampling given a Probability Density Function (PDF) is truly complex. Even sampling from a uniform distribution on $(0,1)$ is a complex problem. In fact, random numbers generated by computers are deterministic. They rely on sequences of numbers generated from a deterministic algorithm. 
Given a seed, the sequence is completely determined.
However, their statistics approximate the statistics of true random numbers. 
Usually, random seeds are selected taking the value of a physical phenomenon that is expected to be random (although probably not uniformly distributed) such as atmospherical noise or CPU temperature.

These algorithms are called pseudorandom number generators. See, e.g., \textcite{knuth1969} for more details. Algorithm \ref{alg_randomNumGen} is a basic example of pseudo-random number generator. Notice that the sequence that it generates is periodic with the periodicity equal to $m$. Even the more complex pseudorandom number generators produce periodic sequences. Although it is named ``random'', notice that given a seed $x_0$, the generated sequence $\{ x_i \}_{i \geq 0}$ is deterministic. 

Algorithm \ref{alg_randomNumGen} does not give a very good approximation of a uniform distribution on the integers from $0$ to $m-1$. However, it is the basic block for building more complex and suitable pseudorandom number generators (see, e.g., \textcite{krauth2006}). The selection of $m$, $n$, and $k$ and the seed $x_0$ drastically affects statistical properties such as mean and variance, and the period length.

\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  \pmb{set:} $m$ (integer), $n$ (integer), $k$ (integer)\;
  \pmb{input:} $x_i$ (integer)\;   
  $x_{i+1} = \textup{mod}(x_i \times n + k, m)$\;
  \pmb{output:} $x_{i+1}$ (integer)
\caption[Basic linear congruential random number generator]
{
  Basic linear congruential pseudorandom number generator. A sequence is generated calling the algorithm several times. Each time the algorithm is called the input will be the output of the previous call. A seed $x_0$ is required. %Noticed that this sequence is periodic with the periodicity equal to $m$. Even the more complex pseudorandom number generators produce periodic sequences. Although it is named ``random'', notice that given a seed $x_0$, the generated sequence $\{ x_i \}_{i \geq 0}$ is deterministic.
}
\label{alg_randomNumGen}
\end{algorithm}
\vspace{5mm}

\subsection{Uniform distribution on $(0,1)$} \label{sec_uniformDistribution}

Given a sequence generated by algorithm \ref{alg_randomNumGen} or any other pseudorandom number generator, it is possible to scale the numbers of the sequence $x_i$ by $\frac{1}{m}$ to be in the interval $(0,1)$, i.e., given the sequence $\{x_i\}_{i \geq 0}$, it is derived the sequence $\{ \frac{x_i}{m} \}_{i \geq 0}$ of numbers belonging to the interval $[0,1)$. 

The goal of this scaling is to generate random numbers according to a uniform distribution in $(0,1)$. Notice that the proposed scaling does not generate samples of a continuous distribution since the set of possible values is finite: $\{0, \frac{1}{m}, \frac{2}{m} , \dots, \frac{m-1}{m} \}$. However, if the pseudorandom number generator approximates a discrete uniform distribution on the integers $\{0, 1, \dots, m-1\}$, then the scaling approximates a continuous uniform distribution when $m \to \infty$. This discretization should not be a major issue when running algorithms in computers. Actually, float numbers\footnote{The representation of real numbers in computers.} are a finite set, and therefore any probability distribution on them is discrete.

\begin{remark}
There is a chance of getting $0$ from the method of generating uniform distributed numbers in $(0,1)$ explained above. As a number belonging to $(0,1)$ is required, the outputs equal to $0$ should be discarded. It can be relevant to avoid overflow, e.g., if functions such as $\log(x)$ or $\frac{1}{x}$ are used.
\end{remark}

\subsection{Inverse transform sampling} \label{sec_inverseTransformSampling}

Section \ref{sec_uniformDistribution} gives a method to generate samples from a uniform distribution on $(0,1)$ when a random or pseudorandom number generator is available. The aim of this section is to give a method to generate samples from an arbitrary distribution when its Cumulative Distribution Function (CDF) and samples from $\textup{uniform}(0,1)$ are available.

\begin{proposition} \label{prop_inverseTransform}
  Let $U \sim \textup{uniform}(0,1)$. Let $F$ be a CDF. Then the random variable $X = F^{-1}(U)$ has $F$ as its CDF.
\begin{proof}
  \begin{equation} \label{eq_inverseTransform}
    \mathbb{P}(X \leq x) = \mathbb{P}(F^{-1}(U) \leq x) = \mathbb{P}(U \leq F(x)) = F(x).
  \end{equation}
\end{proof}
\end{proposition}

\begin{remark} \label{remark_generalizedInverse}
  Proposition \ref{prop_inverseTransform} supposes the existence of the inverse of $F$. CDFs are non-decreasing functions. However, they are not necessarily strictly increasing, and therefore the injectivity may fail. This issue can be tackle defining the inverse of $F$ as,
  $$
    F^{-1}(u) = \inf \{ x \ | \ F(x) \geq u \}
  $$
Thus, for all $u \in (0,1)$ and $x \in F^{-1}((0,1))$,
  $$
    F(F^{-1}(u)) \geq u,
  $$
 and,
  $$
    F^{-1}(F(x)) \leq x.
  $$
Therefore, 
  $$
    \{(u,x) \ | \ F^{-1}(u) \leq x  \} = \{(u,x) \ | \ u \leq F(x) \}
  $$
and (\ref{eq_inverseTransform}) also holds.
\end{remark}

Algorithm \ref{alg_inverseTransform} gives the method derived directly from proposition \ref{prop_inverseTransform}. More details can be found in, e.g., \textcite{robert2004}.

\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  \pmb{input:} $F^{-1}$ (inverse of a CDF)\;   
  \tcp{Generate a random number on $(0,1)$}
  $u = \textup{ran}(0,1)$\;
  \tcp{Evaluate in the inverse of the CDF}
  $x = F^{-1}(u)$\;
  \pmb{output:} $x$\;
\caption[Inverse Transform Sampling]
{
  Inverse Transform Sampling. Method to get samples from a random variable with $F$ as its CDF.
}
\label{alg_inverseTransform}
\end{algorithm}
\vspace{5mm}

\begin{remark} \label{remark_inverseTransformNotWorking}
  The method has two weaknesses. First, it can only be used if the CDF exists. For some probability distributions, it is impossible to compute the CDF analytically\footnote{An important example is the normal distribution. This essential case will be treated in section \ref{sec_samplingGaussian}.}. Thus, this method may be computationally inefficient in the case of such distributions. Secondly, it requires the CDF inverse (or generalized inverse, see remark \ref{remark_generalizedInverse}). Even with the existence proved, the same problem can arise again. It can be difficult or impossible to compute analytically.
\end{remark}

%An important case of probability distribution whose CDF can not be expressed in terms of elementary functions is the normal or Gaussian distribution. 

\subsection{Normal distribution} \label{sec_samplingGaussian}

This section will give a method to sample from a normal distribution.

First, the CDF of a normal distribution $\mathcal{N}(\mu, \sigma^2)$ is
$$
  F(x) = \frac{1}{2}\left( 1 + \textup{erf}\left( \frac{x-\mu}{\sigma\sqrt{2}} \right) \right),
$$
where, 
$$
  \textup{erf}(x) = \frac{1}{\sqrt{\pi}}\int_0^x e^{-t^2}dt.
$$ 
Thus, as stated in remark \ref{remark_inverseTransformNotWorking}, algorithm \ref{alg_inverseTransform} is not suitable for sampling from a normal distribution.

However, recall the PDF of a standard normal distribution,
$$
  f(x) = \frac{1}{2\pi} e^{\frac{-x^2}{2}}, \ x \in \mathbb{R},%(-\infty, \infty),
$$
and notice that,
\begin{equation*}
\begin{aligned}
  &\underbrace{\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}} dx}_{\mathcal{N}(0,1)}
  \underbrace{\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{\frac{-y^2}{2}} dy}_{\mathcal{N}(0,1)}
  = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{1}{2\pi} e^{\frac{-(x^2+y^2)}{2}} dxdy \\
  & \ \ \ \ = \int_{0}^{2\pi} \frac{1}{2\pi} d\theta \int_{0}^{\infty} e^{\frac{-r^2}{2}}rdr 
  \ \ \ \ \ \left( \text{variables change:} 
            \left\{ \begin{array}{c}
              x = \cos(\theta)r  \\
              y = \sin(\theta)r  \\
              dxdy = rd\theta dr \\
            \end{array} \right.
            \right) \\
  & \ \ \ \ = \underbrace{\int_{0}^{2\pi} \frac{1}{2\pi} d\theta}_{\textup{uniform}(0,2\pi)} 
  \underbrace{\int_{0}^{\infty}e^{-\bar{r}}d\bar{r}.}_{\textup{exponential}(\lambda=1)}
  \ \ \ \ \ \text{(variable change: $\bar{r} = \frac{r^2}{2}, rdr = d\bar{r}$)} \\
\end{aligned}
\end{equation*}

Therefore, if
$$
 \begin{aligned}
  \theta  &\sim \textup{uniform}(0,2\pi), \\
  \bar{r} &\sim \textup{exponential}(\lambda = 1), \\
 \end{aligned}
$$
then,
$$
  \begin{aligned}
    x &= \sqrt{2\bar{r}}\cos(\theta) \sim \mathcal{N}(0,1), \\
    y &= \sqrt{2\bar{r}}\sin(\theta) \sim \mathcal{N}(0,1).  \\
  \end{aligned}
$$
Hence, two independent samples from a standard normal distribution can be generated from a sample of a uniform in $(0,2\pi)$ and an exponential with parameter $\lambda=1$.

Generating a sample from a $\textup{uniform}(0,2\pi)$ is straight forward,
$$
  2\pi \times \textup{uniform}(0,1) \sim \textup{uniform}(0,2\pi).
$$
For generating the sample from the exponential, it is possible to use algorithm \ref{alg_inverseTransform}. The CDF is
$$
  F(x) = \int_0^x e^{-\bar{r}}d\bar{r} = 1-e^{-x},
$$
and therefore,
$$
  F^{-1}(u) = -\log(1-u).
$$

\begin{remark} 
  Notice that if $u\sim\textup{uniform}(0,1)$, then $$(1-u) \sim \textup{uniform}(0,1).$$
\end{remark}

The formal proof of the construction above is as follows, if
$$
 \begin{aligned}
  u_1 \sim \textup{uniform}(0,1), \\
  u_2 \sim \textup{uniform}(0,1), \\
 \end{aligned}
$$
and considering the change of variables,
$$
 \left.
 \begin{array}{c}
  x = \sqrt{-2\log(u_2)}\cos(2\pi u_1) \\
  y = \sqrt{-2\log(u_2)}\sin(2\pi u_1) \\
 \end{array} \right\} \Rightarrow
 \begin{array}{l}
  u_1 = \frac{1}{2\pi} \textup{arctan}(\frac{y}{x}) \\
  u_2 = e^{-\frac{x^2+y^2}{2}} \\
 \end{array} 
 \ \ \,
$$
the joint PDF of $x$ and $y$, $f_{xy}$, according to the change of variables rule is
$$
 \begin{aligned}
  f_{xy}(x,y) &= \underbrace{f_{u_1 u_2}\left( u_1(x,y),u_2(x,y) \right)}_{1}
       \underbrace{\lvert \ \textup{J}(x,y) \ \rvert}_{\frac{u_1}{x}\frac{u_2}{y} - \frac{u_1}{y}\frac{u_2}{x}} \\
   &= \frac{1}{2\pi} e^{-\frac{x^2+y^2}{2}} \\
   &= \underbrace{\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}}_{\mathcal{N}(0,1)} \ 
      \underbrace{\frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}}}_{\mathcal{N}(0,1)}.
           & \text{($x$ and $y$ independent)}
 \end{aligned}
$$
where $f_{u_1 u_2}$ is the joint distribution of the uniformly distributed random variables $u_1$ and $u_2$. Note the independence of the random variables $x$ and $y$.

Algorithm \ref{alg_randomNormal} gives the method in pseudocode.

\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  $u_1 = \textup{ran}(0,1)$\;
  $u_2 = \textup{ran}(0,1)$\;
  $\theta = 2\pi u_1$\;
  $\bar{r} = -\log(u_2)$\;
  $x = \sqrt{2\bar{r}}\cos(\theta)$\;
  $y = \sqrt{2\bar{r}}\sin(\theta)$\;
  \pmb{output:} ${x,y}$\;
\caption[Gaussian random numbers using Box-Muller transform]
{
  Gaussian random numbers using Box-Muller transform. Method to get samples from a standard normal distribution.
}
\label{alg_randomNormal}
\end{algorithm}
\vspace{5mm}

This method is called Box-Muller transformation (\cite{box1958}). It can be improved to be marginally faster avoiding calls to the trigonometric functions. Notice that,
$$
  \left.
  \begin{array}{cc}
    r &= \textup{uniform}(0,1) \\
    \phi &= \textup{uniform}(0,2\pi) \\
  \end{array}
  \right\} \Rightarrow
  (r\cos(\phi),r\sin(\phi)) \ \ \substack{\text{random point within} \\ \text{the unit circle.}}  
$$
Thus, it is equivalent to
\begin{equation*}
  \begin{array}{c}
    u_1 = \textup{uniform}(-1,1), \ u_2 = \textup{uniform}(-1,1), \\
    \left. \begin{array}{cc}
      \bar{u}_1 = u_1 \\
      \bar{u}_2 = u_2 \\
    \end{array}
    \right\}
    \text{rejecting $u_1^2 + u_2^2 \geq 1$ and $u_1^2 + u_2^2 = 0$.} \\
  \end{array}
\end{equation*}
and therefore,
\begin{equation*}
  \left. \begin{array}{c}
    r\cos(\phi) = \bar{u}_1 \\
    r\sin(\phi) = \bar{u}_2 \\
  \end{array} \right\} \Rightarrow
  \begin{array}{c}
    \cos(\phi) = \frac{\bar{u}_1}{\sqrt{\bar{u}_1^2 + \bar{u}_2^2}} \\
    \sin(\phi) = \frac{\bar{u}_2}{\sqrt{\bar{u}_1^2 + \bar{u}_2^2}} \\
  \end{array} 
  \ \ \ . 
\end{equation*}
In addition, notice that, %since $u_1$ and $u_2$ are uniformly distributed and only points within the unit circle have been admitted (excluding the origin),
$$
  \bar{u}_1^2 + \bar{u}_2^2 \sim \textup{uniform}(0,1) \Rightarrow
  \bar{r} = -\log(\bar{u}_1^2 + \bar{u}_2^2) \sim \textup{exponential}(\lambda = 1).
$$
\begin{remark}
Proving 
$$
  \bar{u}_1^2 + \bar{u}_2^2 \sim \textup{uniform}(0,1)
$$ 
would require to go into the details of measure theory. Informally, if $(\bar{u}_1, \bar{u}_2)$ are points uniformly distributed within the unit circle and $r=\sqrt{\bar{u}_1^2 + \bar{u}_2^2}$ then,
$$
 \begin{aligned}
  p(r) &= \textup{normalization constant} \times \textup{circumference of radius $r$} \\
    &= \textup{normalization constant} \times 2\pi r \\
    &= \textup{normalization constant} \times r. \\
 \end{aligned}
$$
If $R = r^2$ and applying the change of variables rule,
$$
  f_{R}(R) = f_r(\sqrt{R}) \frac{d(\sqrt{R})}{dR} 
    = \textup{constant} \times \sqrt{R}\frac{1}{2\sqrt{R}} = \textup{constant},
$$
where $f_{R}$ and $f_r$ are the PDFs of $R$ and $r$ respectively.
Therefore,
$$
  R = r^2 = \bar{u}_1^2 + \bar{u}_2^2 \sim \textup{uniform}(0,1).
$$
\end{remark}
Finally,
$$
  \begin{array}{c}
    x = \sqrt{-2\log(\bar{u}_1^2 + \bar{u}_2^2)} \frac{\bar{u}_1}{\sqrt{\bar{u}_1^2 + \bar{u}_2^2}} \ = \sqrt{\frac{-2\log(\bar{u}_1^2 + \bar{u}_2^2)}{\bar{u}_1^2 + \bar{u}_2^2}} \ \bar{u}_1 \ , \\
    y = \sqrt{-2\log(\bar{u}_1^2 + \bar{u}_2^2)} \frac{\bar{u}_1}{\sqrt{\bar{u}_1^2 + \bar{u}_2^2}} \ = \sqrt{\frac{-2\log(\bar{u}_1^2 + \bar{u}_2^2)}{\bar{u}_1^2 + \bar{u}_2^2}} \ \bar{u}_2 \ . \\
  \end{array}
$$
This improvement of the basic Box-Muller transformation (algorithm \ref{alg_randomNormal}) is called Marsaglia polar method (\cite{marsaglia1964}). Algorithm \ref{alg_randomNormalImprovement} gives the pseudocode.

\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  \Do{$r \geq 1$ or $r = 0$}
  {
    $\bar{u}_1 = 2 \times \textup{ran}(0,1) - 1$\;
    $\bar{u}_2 = 2 \times \textup{ran}(0,1) - 1$\;
    $r = \bar{u}_1^2 + \bar{u}_2^2$\;
  }
  $x = \sqrt{\frac{-2\log(r)}{r}} \ \bar{u}_1 \ $\;
  $y = \sqrt{\frac{-2\log(r)}{r}} \ \bar{u}_2 \ $\;
  \pmb{output:} ${x,y}$\;
\caption[Gaussian random numbers using Marsaglia polar method]
{
  Gaussian random numbers using Marsaglia polar method. Improvement of algorithm \ref{alg_randomNormal}. Method to get samples from a standard normal distribution without calls to trigonometric functions.
}
\label{alg_randomNormalImprovement}
\end{algorithm}
\vspace{5mm}

\begin{remark}
  Notice that algorithm \ref{alg_randomNormalImprovement} accepts,
  $$
    \int_{\substack{\text{inside unit} \\ \text{circle}}} du_1 du_2 = \frac{\pi}{4} \approx 78.54 \% 
  $$
  and discards $1-\frac{\pi}{4} = 21.46 \%$ of the uniformly distributed random numbers generated due to the conditional loop, i.e., for every Gaussian sample generated by the algorithm, there are needed $\frac{4}{\pi} \approx 1.27$ uniformly distributed samples. This acceptance rate is sufficiently high to make algorithm \ref{alg_randomNormalImprovement} faster than algorithm \ref{alg_randomNormal} due to the avoidance of expensive trigonometric functions (see \cite{bell1968}). For the same reason, it is also more numerically robust.
\end{remark}

\subsubsection{Non-standard normal distribution} \label{sec_nonstandardnormal}

It has been shown how to generate random samples according to a standard normal distribution (see algorithms \ref{alg_randomNormal} and \ref{alg_randomNormalImprovement}). Those algorithms implicitly give a method to sample from a normal distribution with an arbitrary mean $\mu$ and an arbitrary variance $\sigma^2$ with the simple linear transformation,
$$
  x \sim \mathcal{N}(0,1) \Longrightarrow \mu + \sigma x \sim \mathcal{N}(\mu,\sigma^2).
$$
Proposition \ref{prop_normalTransformation} gives the details.

\begin{proposition} \label{prop_normalTransformation}
If $x \sim \mathcal{N}(0,1)$, then $y = \mu + \sigma x \sim \mathcal{N}(\mu,\sigma^2)$.
\begin{proof}
  Let $f_x$ and $f_y$ be the PDFs of $x$ and $y$ respectively. Applying the change of variables rule,
  $$
    f_y(y) = f_x \left( \frac{y-\mu}{\sigma} \right) \underbrace{\frac{dx}{dy}}_{\frac{1}{\sigma}}
           = \underbrace{\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}}_{\mathcal{N}(\mu,\sigma^2)}
  $$
\end{proof}
\end{proposition}

\subsubsection{Multivariate normal distribution} \label{sec_multivariateNormal}

This section will explain the multivariate case, i.e., sampling from
$$
  x = (x_1, \dots, x_k) \sim \mathcal{N}(\mu, \Sigma),
$$ 
where the mean $\mu$ is a $k$-dimensional vector and the covariance matrix $\Sigma$ is a symmetric positive\hyp{}definite $(k \times k)$-dimensional matrix.

\begin{remark} \label{remark_univariateToMultivariate}
  Notice that if $\Sigma$ is diagonal, then the random variables $\{x_1, \dots, x_k\}$ are independent. Therefore, samples from the random vector $x$ can be generated from independent samples of univariate normal distributions. In particular, a sample from $x \sim \mathcal{N}(0,I_{k \times k})$ can be generated from $k$ independent samples of a univariate standard normal distribution, $x_i \sim \mathcal{N}(0,1)$, using, e.g., algorithm \ref{alg_randomNormal} or algorithm \ref{alg_randomNormalImprovement}.
\end{remark}

\begin{proposition} [Cholesky factorization] \label{prop_cholesky}
  If a matrix $A \in \mathbb{R}^{k \times k}$ is symmetric and is positive definite, then there exists a unique lower triangular matrix $L \in \mathbb{R}^{k \times k}$ with positive diagonal entries such that $A = LL^T$.
\end{proposition}

Proposition \ref{prop_cholesky} is a well-known result in the numerical linear algebra field. Proof and more details can be found in, e.g., \textcite{golub2013}, \textcite{trefethen1997} or \textcite{demmel1997}. In addition to the proof of the existence of $L$, those references also give an algorithm to find it, i.e., to find $L$ given $A$.

\begin{proposition} \label{prop_SigmaSPD}
  Excluding the degenerate case\footnote{When the covariance matrix is singular and the normal distribution has no density.}, the covariance matrix of a multivariate normal distribution $\Sigma \in \mathbb{R}^{k \times k}$ is symmetric and positive\hyp{}definite.
\begin{proof}
  The symmetry is trivial since,
  $$
    \Sigma = \mathbb{E}\left[ (x-\mathbb{E}[x])(x-\mathbb{E}[x])^T \right],
  $$
  and $(x-\mathbb{E}[x])(x-\mathbb{E}[x])^T$ is symmetric.
  To proof the positive\hyp{}definiteness, notice that,
  $$
   \begin{aligned}
    v^T \mathbb{E}\left[ (x-\mathbb{E}[x])(x-\mathbb{E}[x])^T \right] v 
       &= \mathbb{E}\left[ v^T (x-\mathbb{E}[x])(x-\mathbb{E}[x])^T v \right] \\
       &= \mathbb{E}\left[ \left( (x-\mathbb{E}[x])^T v \right)^2 \right]
       \geq 0,
   \end{aligned}
  $$
  for all $v \in \mathbb{R}^k \setminus \{0\}$. Hence, it is positive-semidefinite. Since it is also nonsingular because the degenerate case was excluded, it is positive\hyp{}definite.
\end{proof}
\end{proposition}

By proposition \ref{prop_SigmaSPD}, Cholesky factorization (proposition \ref{prop_cholesky}) can be applied to a covariance matrix, 
$$\Sigma = LL^T,$$ 
with $L$ being lower triangular with positive diagonal entries.

\begin{proposition} \label{prop_linearTransformationNormal}
  Let $A \in \mathbb{R}^{k \times k}$ be a nonsingular matrix and $v \in \mathbb{R}^k$. Let $x \sim \mathcal{N}(\mu, \Sigma)$ with $\Sigma$ nonsingular. The random variable $y = v+Ax$ is distributed according to a normal distribution with mean equal to $v + A\mu$ and covariance matrix $A\Sigma A^T$.
  \begin{proof}
    Notice that $x = A^{-1}(y-v)$ and, since it is a linear transformation, $\lvert \frac{\partial x}{ \partial y} \rvert$ is constant. Applying the change of variables rule, the PDF of $y$ is
    $$
     \begin{aligned}
       f_y(y) &\propto \exp \left( 
                            -\frac{1}{2} 
                            \left( 
                              \left( A^{-1}(y-v)-\mu \right)^T
                              \Sigma^{-1} 
                              \left( A^{-1}(y-v)-\mu \right)
                            \right) 
                          \right) \\
              &\propto \exp \left( 
                            -\frac{1}{2} 
                            \left( 
                              \left( y-(v+A\mu) \right)^T
                              (A^{-1})^T\Sigma^{-1}A^{-1} 
                              \left( y-(v+A\mu) \right)
                            \right) 
                          \right). \\
     \end{aligned}
    $$
  Therefore, the statement holds.
  \end{proof}
\end{proposition}

\begin{corollary} \label{corollary_nonStandardNormalFromStandardNormal}
Let $\Sigma = LL^T$ be the Cholesky factorization (algorithm \ref{prop_cholesky}) of a symmetric and positive\hyp{}definite matrix $\Sigma \in \mathbb{R}^{k \times k}$. Let $\mu \in \mathbb{R}^k$. Let $x \sim \mathcal{N}(0,I_{k \times k})$. The random vector $y = \mu + Lx$ is distributed according to a non-degenerate multivariate normal distribution with mean equal to $\mu$ and covariance matrix equal to $\Sigma$.
\begin{proof}
  The statement is a direct consequence of proposition \ref{prop_linearTransformationNormal}.
\end{proof} 
\end{corollary}

Corollary \ref{corollary_nonStandardNormalFromStandardNormal} gives a method to generate samples from any non-degenerate multivariate normal distribution using samples of a univariate standard normal distribution. 

Given an arbitrary mean $\mu$ and an arbitrary non-singular covariance matrix $\Sigma$, samples $\{y_i\}_i$ from $\mathcal{N}(\mu,\Sigma)$ can be generated from samples $\{x_i\}_i$ of $\mathcal{N}(0,I_{k \times k})$ using the Cholesky factorization $\Sigma = LL^T$ given in proposition \ref{prop_cholesky} and the linear transformation $y_i = \mu + Lx_i$ of corollary \ref{corollary_nonStandardNormalFromStandardNormal}. 
Remember from remark \ref{remark_univariateToMultivariate} that samples from $\mathcal{N}(0,I_{k \times k})$ can be generated from independent samples of a univariate standard normal distribution $\mathcal{N}(0,1)$.
Finally, samples from $\mathcal{N}(0,1)$ can be generated using, e.g., algorithm \ref{alg_randomNormal} or algorithm \ref{alg_randomNormalImprovement}. 

Algorithm \ref{alg_randomMultivariateNormal} gives the pseudocode.

\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  \pmb{inputs:} $\mu = (\mu_1, \dots, \mu_k) \in \mathbb{R}^k$, $ \ \Sigma = \left( \Sigma \right)_{ij} \in \mathbb{R}^{k \times k}$\;
  \tcp{Cholesky factorization}
  \tcp{$L = (L_{ij})_{ij} \in \mathbb{R}^{k \times k}, \text{ with } L_{ij} = 0 \text{ if } j > i$}
  $L = \textup{Cholesky}(\Sigma)$\;
  \For{$i = 1 \dots k$}
  {
    \tcp{sample from an standard normal distribution}
    $x_i = \textup{randn}()$\;
    $y_i = \mu_i$\;
    \For{$j = 1 \dots i$}
    {
      $y_i = y_i + L_{ij} x_j$\;
    }    
  }
  \pmb{output:} $y = (y_1 \dots y_k)$\;
\caption[Random numbers from a multivariate normal distribution]
{
 % Random numbers from a multivariate normal distribution. 
Algorithm to get samples from a multivariate normal distribution $\mathcal{N}(\mu,\Sigma)$. $\textup{Cholesky}(\Sigma)$ returns a matrix $L$ according to the Cholesky factorization $\Sigma = LL^T$ (\ref{prop_cholesky}). $\textup{randn}()$ returns a random number generated from a univariate standard normal distribution (Algorithms \ref{alg_randomNormal} and \ref{alg_randomNormalImprovement} can be used for this purpose). 
}
\label{alg_randomMultivariateNormal}
\end{algorithm}
\vspace{5mm}
 
\subsection{Sampling from an arbitrary PDF} \label{sec_samplingArbitraryPDF}

Methods for sampling from a uniform distribution and from any univariate or multivariate normal distribution were given in sections \ref{sec_uniformDistribution} and \ref{sec_samplingGaussian}. 
Algorithm \ref{alg_inverseTransform} gives a method for sampling from other distributions. However, the drawback of that method is that the inverse of the CDF must be available. 

This section presents methods to sample from an arbitrary PDF. In fact, only a function $f(x)$ proportional to the PDF is necessary.

\subsubsection{Rejection sampling}
Rejection sampling generates independent samples from an arbitrary distribution with PDF $f(x)$ using a proposal distribution with PDF $g(x)$ from which it is possible to sample. 

The basic principle of this method is shown in theorem \ref{theorem_fundamentalTheoremSimulation}.

\begin{theorem}[Fundamental theorem of simulation] \label{theorem_fundamentalTheoremSimulation}
  Sample from 
  $$
    x \sim f(x),
  $$ 
  is equivalent to sample from
  \begin{equation} \label{eq_lawFundTheoSimulation}
    (x,u) \sim \textup{uniform}\left( \left\{ (x,u) \ | \ 0 < u < f(x) \right\} \right),
  \end{equation}
  and taking the values $x$.
  \begin{proof}
    It is sufficient to show that the marginal of $x$ according to the law (\ref{eq_lawFundTheoSimulation}) is $f(x)$,
    $$
      \int_0^{f(x)}du = f(x). 
    $$
  \end{proof}
\end{theorem}

Theorem \ref{theorem_fundamentalTheoremSimulation} and a broaden discussion about it can be found in \textcite{robert2004}. This theorem already gives a method to sample according to an arbitrary density given samples from a uniform distribution. Let $\Omega$ be the sample space of $x$ and $M \geq \sup_x f(x)$. The method consists of taking uniformly distributed samples on
$$
  (x,u) \in \Omega\times(0,M),
$$ 
and rejecting the ones such that $u \geq f(x)$. Figure \ref{fig_RejectionSampling} shows an example. 

\begin{figure}[!htbp]
  \centering
    \includegraphics[width=0.97\textwidth]{codeSomeFigures/RejectionSampling/rejectionSampling.eps}
    \includegraphics[width=0.97\textwidth]{codeSomeFigures/RejectionSampling/rejectionSamplingHistogram.eps}
  \caption[Example of rejection sampling using a uniform proposal distribution.]%
{Example of rejection sampling using a uniform proposal distribution. The figure shows (empirically) that rejection sampling emulates the target density.
  \emph{Black line}: Beta density $f(x)$ with parameters $\alpha = 1.5$ and $\beta = 3$. 
  \emph{Points}: Random points $(x,u)$ uniformly distributed in $(0,1)\times(0,2)$. 
  \emph{Red points}: Accepted points $(x,u)$ satisfying $u < f(x)$. 
  \emph{Blue points}: Rejected points $(x,u)$ satisfying $u \geq f(x)$. 
  \emph{Second graph}: Histogram of the accepted points. 
}
  \label{fig_RejectionSampling}
\end{figure}


The main drawback of applying theorem \ref{theorem_fundamentalTheoremSimulation} directly is that the rejection rate can be so high that makes the algorithm inefficient, especially if the density given by $f(x)$ is concentrated in a small region of $\Omega$.\footnote{A problem that is accentuated in high dimensions.} 

A significant improvement consists of sampling on $\Omega$ from a probability density $g(x)$ that approximates $f(x)$ as much as possible and from which it is known how to obtain samples, instead of sampling from $\textup{uniform}(\Omega)$. The principle is the same than theorem \ref{theorem_fundamentalTheoremSimulation}. 

Let $u \in (0,1)$ and consider the following join density in the pairs $(x,u)$,
\begin{equation} \label{eq_densityRejectionSampling}
  p(x,u) = \left\{ \begin{array}{cc}
    M g(x), \ &u < \frac{f(x)}{Mg(x)} \\
    0, &u \geq \frac{f(x)}{Mg(x)} \\
  \end{array} \right. \ \ ,
\end{equation}
where $M \in \mathbb{R}$ is a constant such that $Mg(x) > f(x)$ for all $x$ in the sample space $\Omega$. The density (\ref{eq_densityRejectionSampling}) is well defined since,
$$
  \int_{\Omega} \int_0^1 p(x,u)dudx
    = \int_{\Omega} \int_0^{\frac{f(x)}{Mg(x)}} Mg(x) du dx
    = \int_{\Omega} f(x) dx = 1
$$
Notice that the marginal coincides with $f(x)$,
\begin{equation} \label{eq_rejectionSampleMarginal}
  p(x) = \int_0^{\frac{f(x)}{Mg(x)}} p(x,u) du
       = \int_0^{\frac{f(x)}{Mg(x)}} M g(x) du = f(x).
\end{equation}
Let $p_2(x,u) = p_2(x)p_2(u)$ be another joint density on the pairs $(x,u)$ with $x$ and $u$ independent, $p_2(x) = g(x)$ and $p_2(u) = 1$. Thus, samples of $(x,u)$ according to $p_2$ can be generated indepedently from samples,
\begin{equation} \label{eq_rejectionSampleSampling}
  \begin{aligned}
    x &\sim g(x), \\
    u &\sim \textup{uniform}(0,1).
  \end{aligned}
\end{equation}
Notice that,
$$
\begin{aligned}
  p_2\left((x,u) \ | \ (x,u) \in \left\{ (x,u) | u < \frac{f(x)}{Mg(x)} \right\} \right) = \\
 \begin{array}{r} \begin{aligned}
    &= \frac{p_2(x,u)}{ p_2\left((x,u) \in \left\{ (x,u) | u < \frac{f(x)}{Mg(x)}\right\} \right) } \\
    &= \frac{g(x)}{\int_{\Omega} \int_0^{\frac{f(x)}{Mg(x)}}g(x)dudx} \\
    &= Mg(x). \\
 \end{aligned} \end{array}
\end{aligned}
$$
Therefore, for sampling from \ref{eq_densityRejectionSampling}, it is only necessary to sample independently according to \ref{eq_rejectionSampleSampling} and discard the pairs $\left\{ (x,y) | u \geq  \frac{f(x)}{Mg(x)} \right\}$. Finally, as it is proved in \ref{eq_rejectionSampleMarginal}, taking only the values $x$ leads to the law defined by the density $f(x)$.

Refer to \textcite{robert2004} for more information about rejection rampling. Algorithm \ref{alg_rejectionSampling} gives the pseudocode.

\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  \pmb{inputs:} target density $f(x)$, proposal density $g(x)$ and constant $M$ such that $Mg(x) > f(x), \ \forall x$\;
  \Do{$u \geq \frac{f(x)}{Mg(x)}$}
  {
    \tcp{get samples according to $g(x)$ and $\textup{uniform}(0,1)$}
    $x = \textup{ran}_{\sim g(x)}()$\;
    $u = \textup{ran}(0,1)$\;
  }
  \pmb{output:} $x$\;
\caption[Rejection sampling]
{
  Rejection sampling. Algorithm to sample according to the target PDF $\propto f(x)$ from a proposal $g(x)$, assuming that samples according to $g(x)$ can be generated.
}
\label{alg_rejectionSampling}
\end{algorithm}
\vspace{5mm}

\begin{remark}
  Notice that, actually, if $f(x)$ is not normalized the method still works.
\end{remark}

\begin{remark}
  It can be shown (see, e.g., \cite{robert2004}) that if,
  $$
   \begin{aligned}  
    x &\sim g(x), \\
    u &\sim \textup{uniform}(0,1), \\
   \end{aligned}
  $$
  then,
  $$
    \mathbb{P}\left( u < \frac{f(x)}{Mg(x)} \right) = \frac{1}{M}. \\
  $$
  Therefore, the rejection sampling method is optimized by setting,
  $$
    M = \underset{x}{\textup{sup}} \frac{f(x)}{g(x)}.
  $$
\end{remark}

\begin{remark}
  Improvements of rejection sampling method focus on finding suitable proposals $g(x)$ which lead to a low rejection rate.
\end{remark}

A major drawback of rejection sampling is that it is difficult to find a proposal that leads to a good acceptance rate in high dimensions. The density of the target PDF is concentrated in a region of the sample space and many rejections are needed until a sample with high acceptance rate is generated.

Fortunately, there is a family of methods called Markov Chain Monte Carlo (MCMC) algorithms that can deal with densities in high-dimensions. However, in contrast to rejection sampling, samples generated by MCMC algorithms are correlated. This is a problem inherent in the MCMC sampling procedure.

\subsubsection{Markov chain sampling} \label{sec_markovChain}

Given a function $f(x)$ proportional to a desired probability density, the aim is to develop an algorithm able to sample according to that density, even in a high dimensional scenario.

There is a family of algorithms based on Markov chains capable to tackle this problem. The theory of Markov chains is beyond the scope of this work. Nevertheless, a simple example in a discrete sample space will be given. It will be sufficient to understand the basic principle on which Markov chain sampling algorithms are based. 

Refer to \textcite{krauth2006} to broaden some of the ideas given below. Refer to \textcite{robert2004} for a deep understanding of Markov chains.

Let $x$ be a discrete variable which can take $9$ values,
$$
  x \in \Omega = \{1,2,3,4,5,6,7,8,9\}.
$$
Let $p_s$ be a probability distribution in the sample space $\Omega$, e.g.,
$p_s(x) = \frac{1}{9}$ for all $x\in\Omega$.
The distribution $p_s$ will be called stationary distribution. 
Suppose that it is required to generate samples according to $p_s$.
Obviously, in this example, it is possible to use a random number generator (see alrgorithm \ref{alg_randomNumGen}). This approach is called ``direct sampling''. However, remember that the goal is to sample according to densities from which there is not a direct method to generate samples. Instead of doing that, consider the following approach, 
\begin{enumerate}
  \item Select a value $x_0$ in the sample space $\Omega$ (randomly according to a known distribution or deterministically).
  \item Assign transition probabilities $\{p(x \rightarrow x')\}_{x,x' \in \Omega}$ to move from a value $x \in \Omega$ to another value $x' \in \Omega$ (or staying in the same value).
  \item Generate a chain $\{x_0, x_1, x_2, \dots\}$ starting at $x_0$ and moving according to the transition probabilities $p(x \rightarrow x')$\footnote{The series $\{x_0, x_1, \dots \}$ is called Markov chain and this process of moving around the sample space $\Omega$ is called random walk.}.
\end{enumerate}
Is there any selection of transition probabilities $\{p(x \rightarrow x')\}_{x,x' \in \Omega}$ such that, after several movements, the probability of being in a value $x$ is the same (or very close) to the stationary probability $p_s(x)$ for all $x \in \Omega$?. 

The answer to this question, i.e., a suitable choice of transition probabilities, is precisely what allows Metropolis-Hasting (M-H) to sample according to any given probability density $f(x)$, even if $f(x)$ is not normalized\footnote{$f(x)$ is proportional to a probability density which it is wanted to simulate, i.e., $f(x)$ is not a PDf but $\textup{C}\times f(x)$ is a PDF for a normalizing constant $C$.} or the random variable $x$ is continuous, moving around the sample space $\Omega$. The details of M-H shall be explained in next section \ref{sec_MetropolisHastings}.

Consider that the values in the chain $\{x_1, x_2, \dots\}$ are random variables distributed according to the initial value $x_0$ and the random walk given by the transition probabilities $\{p(x \rightarrow x')\}_{x,x' \in \Omega}$.
 
If $\{p(x \rightarrow x')\}_{x,x' \in \Omega}$ are transition probabilities satisfying the stated question, then the probability densities $\{p_i = p(x_i)\}_{i>1}$ associated to the random variables $\{x_i\}_{i>1}$ tend to $p_s(x)$ when $i \to \infty$. Thus, intuitively, in the limit, the following condition must be necessary,
$$
  p_s(x) = \sum_{x'=1}^9 p_s(x')p(x' \rightarrow x),
$$  
for all $x \in \Omega$. Therefore,
\begin{equation} \label{eq_Mchain1}
  p_s(x)\left( 1 - p(x \rightarrow x) \right) = \sum_{\substack{x' \in \Omega \\ x' \neq x}} p_x(x')p(x' \rightarrow x), 
\end{equation}
and, since,
$$
  1 - p(x \rightarrow x) = \sum_{\substack{x' \in \Omega \\ x' \neq x}} p(x \rightarrow x'),
$$
equality (\ref{eq_Mchain1}) leads to
\begin{equation} \label{eq_Mchain2}
  \sum_{\substack{x' \in \Omega \\ x' \neq x}} p_s(x)p(x \rightarrow x') = \sum_{\substack{x' \in \Omega \\ x' \neq x}} p_x(x')p(x' \rightarrow x).
\end{equation}
Notice that a sufficient condition (but not necessary) for $\ref{eq_Mchain2}$ is,
\begin{equation} \label{eq_detailedBalance}
  p_s(x)p(x \rightarrow x') = p_s(x')p(x' \rightarrow x),
\end{equation}
for all pairs $x, x' \in \Omega$.
Condition (\ref{eq_detailedBalance}) is usually called ``detailed balance'' in the Markov chains literature. It is also called ``reversibility'', ``microscopic reversibility'' or ``time reversibility'' by some authors. 
See, e.g., \textcite{chib1995}.
It is the key of designing sampling methods based on Markov chains (see M-H in next section \ref{sec_MetropolisHastings}) because it is a sufficient condition\footnote{Assuming the ergodicity of the Markov process. See \textcite{robert2004}.} for the Markov process to asymptotically reach a unique stationary distribution which, in addition, coincides with the desired distribution. 

It will be constructed transition probabilities $\{p(x \rightarrow x')\}_{x,x' \in \Omega}$ for the example above.
For a better visualization of the random walk, consider the values in $\Omega$ to be in a grid,
\begin{center}
  \includegraphics[width=0.2\textwidth]{MarkovChainExampleGameDrawings/numbers.eps}
\end{center}
and transition probabilities allowing only the following movements,
\begin{center}
\begin{tabular}{c c c}
  \underline{from the center} & \underline{from a corner} & \underline{from a side} \\
  \includegraphics[width=0.2\textwidth]{MarkovChainExampleGameDrawings/center.eps} &
  \includegraphics[width=0.2\textwidth]{MarkovChainExampleGameDrawings/corner.eps} &
  \includegraphics[width=0.2\textwidth]{MarkovChainExampleGameDrawings/side.eps} \\
\end{tabular}
\end{center}
or staying in the same value. 

For example, the transition probabilities from the corner $1$ are,
$$
  \begin{aligned}
    p(1 \rightarrow 1) &\geq 0, \\
    p(1 \rightarrow 2) &\geq 0, \\
    p(1 \rightarrow 4) &\geq 0, \\
    p(1 \rightarrow x) &= 0, \ \forall x \in \{3,5,6,7,8,9\}. \\
  \end{aligned}
$$
Therefore, from the value $x_i = 1$, the process can stay at $1$, move to $2$ or move to $4$, i.e., $x_{i+1}$ can take the values $1$, $2$ or $4$.
In addition,
\begin{equation} \label{eq_Mchain3}
  \sum_{x=1}^9 p(1 \rightarrow x) = p(1 \rightarrow 1) + p(1 \rightarrow 2) + p(1 \rightarrow 4) = 1,
\end{equation}
since $p(1 \rightarrow x), x \in \Omega,$ is a probability distribution itself.
Consider a desired distribution given by $p_s(x), x \in \Omega,$ from which it is wanted to sample. 
Following the reasoning explained above, the procedure will be to move around the sample space $\Omega$, i.e., around the grid, with the aim of approximating $p_s$ after several moves.
Therefore, in the limit, when the number of movements tends to $\infty$, the following condition must be satisfied,
\begin{equation} \label{eq_Mchain4}
  p_s(1) = p_s(1)p(1 \rightarrow 1) + p_s(2)p(2 \rightarrow 1) + p_s(4)p(4 \rightarrow 1),  
\end{equation}
since $1$ can only be reached from $2$, $4$ and $1$ itself.
Hence, from (\ref{eq_Mchain3}) and (\ref{eq_Mchain4}),
$$
  p_s(1)p(1 \rightarrow 2) + p_s(1)p(1 \rightarrow 4) = p_s(2)p(2 \rightarrow 1) + p_s(4)p(4 \rightarrow 1),
$$
condition that can be satisfied imposing the detailed balance,
$$
  p_s(1)p(1 \rightarrow 2) = p_s(2)p(2 \rightarrow 1),
$$
and,
$$
  p_s(1)p(1 \rightarrow 4) = p_s(4)p(4 \rightarrow 1).
$$
Notice that the same construction can be done with the other values, $2, 3, \dots, 9$.

Let $p_s(x) = \frac{1}{9}$, for all $x\in\Omega$, be the desired distribution.
In this case, when $p_s$ is uniform in $\Omega$, the detailed balance condition reduces to 
$$
  p(x \rightarrow x') = p(x' \rightarrow x),
%  p(1 \rightarrow 2) = p(2 \rightarrow 1) \ \text{ and } \
%  p(1 \rightarrow 4) = p(4 \rightarrow 1).
$$
for all $x,x' \in \Omega$.

If the transition probabilities are, e.g., $p(x \rightarrow x') = \frac{1}{4}, \ x \neq x'$, for the allowed movements,
$$
 \begin{aligned}
  p(1 \rightarrow 1) &= 1 - p(1 \rightarrow 2) - p(1 \rightarrow 4) = 1 - \frac{1}{4} - \frac{1}{4} = \frac{1}{2}, \\
  p(2 \rightarrow 2) &= 1 - p(2 \rightarrow 1) - p(2 \rightarrow 5) - p(2 \rightarrow 3) = 1 - \frac{3}{4} = \frac{1}{4}, \\
  p(3 \rightarrow 3) &= \dots = \frac{1}{2}, \\
  p(4 \rightarrow 4) &= \dots = \frac{1}{4}, \\
  p(5 \rightarrow 5) &= \dots = 0, \\
  p(6 \rightarrow 6) &= \dots = \frac{1}{4}, \\
  p(7 \rightarrow 7) &= \dots = \frac{1}{2}, \\
  p(8 \rightarrow 8) &= \dots = \frac{1}{4}, \\
  p(9 \rightarrow 9) &= \dots = \frac{1}{2}, \\
 \end{aligned}
$$
and $p(x \rightarrow x') = 0$ for the other cases, then the detailed balance is satisfied. 

In the discrete case, the transition probabilities can be shown in a matrix,
\begin{equation} \label{eq_MchainTransitionMatrix}
  K = \left( p(i \rightarrow j) \right)_{ij} = 
  \begin{pmatrix}
    \frac{1}{2} & \frac{1}{4} & 0 & \frac{1}{4} & 0 & 0 & 0 & 0 & 0 \\
    \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & 0 & \frac{1}{4} & 0 & 0 & 0 & 0 \\
    0 & \frac{1}{4} & \frac{1}{2} & 0 & 0 & \frac{1}{4} & 0 & 0 & 0 \\
    \frac{1}{4} & 0 & 0 & \frac{1}{4} & \frac{1}{4} & 0 & \frac{1}{4} & 0 & 0 \\
    0 & \frac{1}{4} & 0 & \frac{1}{4} & 0 & \frac{1}{4} & 0 & \frac{1}{4} & 0 \\
    0 & 0 & \frac{1}{4} & 0 & \frac{1}{4} & \frac{1}{4} & 0 & 0 & \frac{1}{4} \\
    0 & 0 & 0 & \frac{1}{4} & 0 & 0 & \frac{1}{2} & \frac{1}{4} & 0 \\
    0 & 0 & 0 & 0 & \frac{1}{4} & 0 & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} \\
    0 & 0 & 0 & 0 & 0 & \frac{1}{4} & 0 & \frac{1}{4} & \frac{1}{2} \\
  \end{pmatrix} \ .
\end{equation}

Notice that a distribution in $\Omega$ can be represented by a vector,
$$
  v_p = \left( p(1), p(2), \dots, p(9) \right)^T.
$$
Following this notation,
$$
  v_{p_s} = \left(\frac{1}{9}, \frac{1}{9}, \dots, \frac{1}{9}\right)^T.
$$
Let $p_0 = p(x_0)$ be the distribution of the intial value $x_0$. 
Remember that the choice of $x_0$ can be deterministic, e.g., if it is wanted to impose $x_0 = 3$, then
$$
  v_{p_0} = (0,0,1,0,0,0,0,0,0)^T.
$$
Define $p_i = p(x_i)$, i.e., the distribution of the random variable $x_i$ (the value after $i$ movements).
Note that 
$$
  v_{p_{i}} = Kv_{p_{i-1}} = K^i v_{p_0}.
$$
Eventually, in order to meet the requirement stated above and tending to the desired density, 
\begin{equation} \label{eq_Mchain5}
  K^i v_{p_0} \xrightarrow{i \to \infty} v_{p_s},
\end{equation}
should hold.

\begin{remark}
Indeed, condition (\ref{eq_Mchain5}) is true for any initial distribution $p_0$. Studying the spectrum of $K$,
\footnote{Refer to the power method (e.g. \cite{golub2013}) for more details about the connection between $K^i p_0$ and the eigenvalues and eigenvectors of $K$.}
it can be seen that the dominant eigenvalue is $1$ and an eigenvector associated to it is
$$
  (1,1,1,1,1,1,1,1)^T.  
%with correspondig eigenvalue equal to $1$. 
$$ 
\end{remark}

\begin{remark}
  The second largest eigenvalue of $K$ is $0.75$. Thus, the error between $p_s$ and the asymptotic approximation $p_i$ is given by $\approx 0.75^i u_2$, where $u_2$ is an eigenvector of eigenvalue $0.75$.
\end{remark}

\subsubsection{Metropolis Hastings} \label{sec_MetropolisHastings}

This section will explain M-H sampling algorithm. First, the idea of building a Markov chain $\{x_0,x_1,\dots\}$ from a random walk around the sample space introduced in last section \ref{sec_markovChain} will be extended to the continuous case. Secondly, a methodology to make transitions which satisfy the detailed balance condition (\ref{eq_detailedBalance}) will be given. Finally, a pseudocode of the algorithm will be given.

Remember that transition probabilities can be shown in a matrix in the discrete case (see (\ref{eq_MchainTransitionMatrix})). The following definition will extend this concept to the continuous case, where this matrix is infinite-dimensional and will be given by a function called transition kernel.

\begin{definition}[Transition kernel] \label{def_transitionKernel}
  Let $\Omega \subset \mathbb{R}^l$, for some positive integer $l$, be a sample space. A transition kernel is a function $K$ defined on $\Omega \times \mathcal{B}(\Omega)$ such that $K(x,\cdot)$ is a probability measure on $\mathcal{B}(\Omega)$.
\end{definition}

%\begin{remark}
%  The transition probabilities in a random walk according to a transition kernel $K$ are straightforward. 
%  The probability of $\mathcal{A} \subset \mathcal{B}(\Omega)$ from the value or state $x \in \Omega$ is $K(x,\mathcal{A})$.     
%\end{remark}

\begin{remark}
  In the discrete case, the probability of the transition from $x\in\Omega$ to $y\in\Omega$ is given by the element $K_{x,y}$ of the transition matrix.
  In the continuous case, the transition probability from $x\in\Omega$ to $\mathcal{A}\in\mathcal{B}(\Omega)$ is given by the transition kernel evaluation $K(x,\mathcal{A})$.
\end{remark}

As it has been done in the example where $\Omega$ was discrete (see last section \ref{sec_markovChain}), the probability that the chain remains at the same point can be strictly positive. 
Consider 
$$
 \begin{aligned}
  t: \Omega^2 &\to \mathbb{R}^+ \\
     (x,y)   &\mapsto t(x,y) \\
 \end{aligned}
$$
and
$$
 \begin{aligned}
  r: \Omega^2 &\to \mathbb{R}^+ \\
         x    &\mapsto r(x) \\
 \end{aligned} 
$$
such that,
\begin{equation} \label{eq_transitionKernelDescomposition}
  K(x,dy) = t(x,y)dy + r(x)\delta_x(dy),
\end{equation}
where $\delta_x(dy) = 1$ if $x\in dy$ and $0$ otherwise.

\begin{remark}
 Note that $\int_{\Omega} t(x,y)dy < 1$ if $r(x) > 0$. Therefore, $t(x, \cdot)$ is not necessarily a probability density function.
\end{remark}

\begin{remark}
Notice that
\begin{equation} \label{eq_MHrx}
  r(x) = 1 - \int_{\Omega} t(x,y)dy
\end{equation}
is the probability that the chain remains at $x$.
\end{remark}

Define the detailed balance condition for a continuous probability density $f(x), x\in\Omega$ and a transition kernel (\ref{eq_transitionKernelDescomposition}) by
\begin{equation} \label{eq_detailedBalanceContinuous}
  f(x)t(x,y) = f(y)t(y,x)
\end{equation}

\begin{proposition} \label{prop_detailedBalanceContinuous}
  If a transition kernel $K$ (definition \ref{def_transitionKernel}) satisfies the detailed balance condition (\ref{eq_detailedBalanceContinuous}) for a probability density $f(x)$, then $f(x)$ is the stationary density of $K$.
  \begin{proof}
   Note that,
   $$
    \begin{aligned}
      \int_{\Omega} K(x,\mathcal{A})f(x)dx &= \int_{\Omega} 
        \left( \int_{\mathcal{A}} t(x,y)dy + r(x)\delta_x(dy) \right) f(x) dx \\
        &= \int_{\Omega} \int_{\mathcal{A}} t(x,y)f(x) dy dx
          + \int_{\Omega} \delta_x(\mathcal{A}) r(x)f(x) dx \\
        &= \int_{\mathcal{A}} \int_{\Omega} t(x,y)f(x) dx dy
          + \int_{\mathcal{A}} r(x)f(x)dx \\
        &= \int_{\mathcal{A}} \int_{\Omega} \underbrace{t(y,x)f(y)}_{\text{detailed balance}} dx dy
          + \int_{\mathcal{A}} r(x)f(x)dx \\
              %        & \omit\hfill \text{(detailed balance)} \\
        &= \int_{\mathcal{A}} (1-r(y))f(y) dy
          + \int_{\mathcal{A}} r(x)f(x)dx & \text{(\ref{eq_MHrx})} \\
        &= \int_{\mathcal{A}} f(y)dy,
    \end{aligned}
   $$
   for any $\mathcal{A} \in \mathcal{B}(\Omega)$.
  \end{proof}
\end{proposition}

Let $\{g(x,\cdot)\}_{x\in\Omega}$ be a set of probability density functions indexed by $x$. 
Suppose that samples according to those densities can be generated.
Those PDFs will be called proposal PDFs.

For a desired PDF $f(x)$ consider the problem of finding 
$$
 \begin{aligned}
  \alpha: \Omega^2 &\to [0,1] \\
             (x,y) &\mapsto \alpha(x,y),
 \end{aligned}
$$
such that,
$$
  t(x,y) = g(x,y)\alpha(x,y),
$$
satisfies the detailed balance condition (\ref{eq_detailedBalanceContinuous}).

If the proposal densities $g(x,y)$ already satisfies the detailed balance, then setting $t(x,y) = g(x,y)$ and $r(x)=0, \forall x\in\Omega$ will solve the problem, i.e., choosing a starting point $x_0$ and making transition according to $x_{i+1} \sim g(x_i,\cdot)$ will converge to the desired PDF.
Unfortunetely, in general, $t(x,y) = g(x,y)$ would not satisfy the detailed balance. 
\textcite{metropolis1953} propose the following choice,
\begin{equation} \label{eq_metropolisChoice}
  \alpha(x,y) = \min\left( \frac{f(y)g(y,x)}{f(x)g(x,y)} , 1 \right),
\end{equation}
which is the key of M-H algorithm.

\begin{remark}
  It is assumed that $\Omega$ is set such that $f(x) > 0, \forall x\in\Omega$, and $g(x,y) > 0, \forall x,y\in\Omega$. If not, Metropolis choice should be,
$$
  \alpha(x,y) = \left\{ \begin{array}{cc}
    \min\left( \frac{f(y)g(y,x)}{f(x)g(x,y)} , 1 \right), & \text{ if } \ f(x)g(x,y) > 0, \\
      0, & \text{otherwise.}  \\
  \end{array} \right. 
$$ 
\end{remark}

\begin{remark}
  Using Metropolis transitions, $f(x)$ does not need to be normalized because of the cancelation in the quotient $\frac{f(y)g(y,x)}{f(x)g(x,y)}$.
\end{remark}

\begin{proposition} \label{prop_metropolisChoiceJustified}
  Let $f(x), x\in\Omega,$ be a function proportional to a desired PDF. 
  Let $\{g(x,\cdot)\}_{x\in\Omega}$ be a collection of proposal densities indexed by $x$.
  If
  \begin{equation} \label{eq_MetropolisWorks2}
    \alpha(x,y) = \min\left( \frac{f(y)g(y,x)}{f(x)g(x,y)} , 1 \right),
  \end{equation}
  then the desired PDF (proportional to $f(x)$) is the stationary density of the transition kernel (\ref{eq_transitionKernelDescomposition}) according to 
  \begin{equation} \label{eq_MetropolisWorks1}
    t(x,y) = g(x,y)\alpha(x,y).
  \end{equation}
  \begin{proof}
    By proposition \ref{prop_detailedBalanceContinuous}, it is sufficient to prove the detailed balance condition (\ref{eq_detailedBalanceContinuous}). Substituting according to (\ref{eq_MetropolisWorks1}), it can be rewritten as
    $$
      \frac{g(x,y)f(x)}{g(y,x)f(y)} = \frac{\alpha(y,x)}{\alpha(x,y)},
    $$
    which is satisfied by the choice of $\alpha$ (\ref{eq_MetropolisWorks2}).
  \end{proof}
\end{proposition}

M-H uses Metropolis choice of $\alpha$ (\ref{eq_metropolisChoice}). Proposition \ref{prop_metropolisChoiceJustified} justifies this choice. Algorithm \ref{alg_MH} shows the pseudocode.

\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  \pmb{inputs:} Function $f(x)$ proportional to a PDF; proposal densities $g(x,\cdot)$; maximum number of iterations $n_{max}$; initial state $x_0$\;
  \For{$i=1,\cdots,n_{max}$}
  {
    \tcp{Sample from $g(x_{i-1},\cdot)$}
    $y \sim g(x_{i-1},\cdot)$\;
    \tcp{Sample from a uniform distribution}
    $u = \textup{ran}(0,1)$\;
    \tcp{Compute the acceptance ratio}
    $a = \min\left( \frac{f(y)g(y,x)}{f(x)g(x,y)} , 1 \right)$\;
    \uIf{$u \leq a$}
    {
      \tcp{Accept}
      $x_i = y$\;
    }
    \Else
    {
      \tcp{Reject}
      $x_i = x_{i-1}$\;
    }
  }
  \pmb{output:} $\{x_0,x_1,x_2,\cdots\}$\;
\caption[Metropolis-Hastings (M-H) algorithm]
{
  Metropolis-Hastings (M-H) algorithm. Method to sample according to the target PDF $\propto f(x)$ from proposals $g(x,\cdot)$. It assumes that samples according to $g(x,\cdot)$, for any $x$ in the sample space, can be generated.
}
\label{alg_MH}
\end{algorithm}
\vspace{5mm}

\begin{remark}
  Notice the connection between the acceptance-rejection step in M-H (algorithm \ref{alg_MH}) and the acceptance-rejection sampling (algorithm \ref{alg_rejectionSampling}).
\end{remark}

\begin{remark}
  Usually, a number of points at the beginning of the chain are discarded after running M-H (algorithm \ref{alg_MH}). This is to reduce the effect of correlation with the fixed initial point. Those first iterations are called burning period.
\end{remark}

Proposition \ref{prop_metropolisChoiceJustified} is not sufficient proof for the convergence to the desired density. 
Full proof of convergence needs the use of Markov chain theory. As mentioned in last section \ref{sec_markovChain}, it is out of the scope of this work and only justification for the detailed balance is given. 
The other necessary conditions are irreducibility and aperiodicity. 
Informally, the first one means that the chain must be able to move from any area of the sample space to any other area of the sample space in a finite number of moves with non-zero probability. 
The second one means that this finite number of moves must not be required to be a multiple of some integer.
The fulfillment of both conditions is called ergodicity.
Ergodicity can be satisfied in M-H by setting proposal densities with strictly positive values on any point of $\Omega$.
More details about the theory can be found in, e.g., \textcite{robert2004}, \textcite{chib1995} or \textcite{smith1993}. 

Improvements of M-H focus on finding proposal densities $g(x,\cdot)$ which ensure a faster convergence. 
For example, adaptative M-H tune the parameters of the proposal densities according to the chain history during the algorithm run, taking special care in not breaking the ergodicity condition (see \cite{haario2001}).

In addition to that, there are other improvements. 
Two important algorithms related to M-H\footnote{In fact, they are particular cases of M-H.} are Gibbs sampling (see, e.g., \cite{casella1992}) and Hamiltonian Monte Carlo (HMC) (see, e.g., \cite{betancourt2018}; or \cite{brooks2011}).  

Gibbs sampling uses the conditional probabilities when they are easier to simulate.

HMC uses hamiltonian dynamics to reduce correlation between successive samples\footnote{It is specially useful for computing integrals because a fewer number of samples are needed for obtaining a good approximation.}.
However, the derivatives are needed to run HMC due to the hamiltonian procedure, and therefore it is unsuitable when they are not available. 


\subsection{Sampling with variables' constraints} \label{sec_samplingWithVariablesConstraints}

This section will give a method to use the sampling algorithms exposed in this Chapter when there are constraints in the random variables.
The ideas of this section about sampling from a manifold given by variables constraints are novel ideas of this thesis. 
They are based on measure theory.
Sampling with variables' constraints can be relevant, e.g., when designing proposal distributions (see section \ref{sec_MetropolisHastings}) in constrained spaces and satisfying the detailed balance condition (see \ref{eq_detailedBalanceContinuous}).
Or, from another perspective, conditioning a random variable to be in a manifold
%This section will give a method to work with the probability distribution of a random variable with a known PDF in the ambient space conditioned to belong to a constrained space. 
\footnote{A necessary condition will be that this constrained space must be a manifold.}
(see remark \ref{remark_constraintsInDesignVariables}).
This problem was motivated by the mixture weight space (see (\ref{eq_mixtureWeightsSpace})) that will be presented in section \ref{sec_mixtures}.

Let $f(x)$ be the PDF of a random variable 
$$x = (x_1, \dots, x_m) \in \Omega \subset \mathbb{R}^m.$$

\begin{remark}
  Remember the abuse of notation explained in section \ref{sec_probMeasuresNotation}. There is no distinction in notation between the random variable $x$ and the variable of the PDF $f$.
\end{remark}

Let $\mathcal{A} \subset \Omega$ be a $k$-dimensional manifold in $\mathbb{R}^m$ with $k<m$.
This manifold can be given, e.g., by variables' constraints, $g(x) = 0$.

\subsubsection{A generalization of the conditional PDF}

The aim is sampling according to the PDF $f(x)$ and conditioning to $x\in\mathcal{A}$.

The measure-theoretic approach of probability theory is necessary for tackling this problem. %has not been introduced in this work.
%However, it is necessary for tackling this problem.
See, e.g., \textcite{bobrowski2005} for more details measure about the connection between measure theory and probability theory.

Let $(\Omega \subset \mathbb{R}^m, \mathcal{B}(\Omega), \mathbb{P})$ be the probability space of the random variable $x$.
As usual, if there are no indications, PDFs defined in this work are defined w.r.t. the Lebesgue measure in $\mathbb{R}^m$, in the sense that
$$
  \mathbb{P}(x \in A) = \int_A d\mathbb{P}(x) = \int_A f(x)dx.
$$

Notice that the Lebesgue measure of $\mathcal{A}$ is equal to $0$ since $k<m$.
Therefore, $\mathbb{P}(x \in \mathcal{A}) = 0$.
Conditioning to events of probability $0$ is not trivial.
BorelKolmogorov paradox is an example (see, e.g., \textcite{rescorla2015}).

\begin{remark} \label{remark_usualConditionalProbability}
Let $x=(x_1,x_2) \in \mathbb{R}^2$ be a random variable with PDF $f_{x}$ w.r.t. the Lebesgue measure in $\mathbb{R}^2$. Note that the PDF $f_{x_1|x_2}(x_1)$ of $x_1$ conditioned to $x_2 = a$ is well known. It is
$$
  p(x_1 | x_2=a) = \frac{p(x_1,a)}{\int_{\mathbb{R}}p(x_1,a)dx_1}
$$
w.r.t. the Lebesgue mesure in $\mathbb{R}$. 
And note that the Lebesgue measure in $\mathbb{R}^2$ of 
$$
  \left\{ (x_1,x_2)\in\mathbb{R}^2 | x_2 = a \right\}
$$
is $0$.
\end{remark}

\begin{definition} \label{def_conditionalManifold}
Let $x = (x_1,\dots,x_m)\in\Omega\subset\mathbb{R}^m$ be a random variable. Let $\mathcal{A}\subset \Omega$ be a manifold in $\mathbb{R}^m$. Let $f_x(x)$ be the PDF of $x$ w.r.t. the Lebesgue measure in $\mathbb{R}^m$. Let $\mu$ be the Lebesgue measure in the manifold $\mathcal{A}$. Define the conditional PDF $f_{x|x\in\mathcal{A}}(x)$ of $x$ conditioned to $x\in\mathcal{A}$ w.r.t $\mu$ as
\begin{equation} \label{eq_conditionalGeneral}
  f_{x | x\in\mathcal{A}}(x) = \frac{f_x(x)}{\int_{\mathcal{A}} f_x(x) d\mu(x)}.
\end{equation}
\end{definition}

\begin{remark}
Note that the PDF (\ref{eq_conditionalGeneral}) is defined w.r.t. $\mu$, the Lebesgue measure in the manifold $\mathcal{A}$. Therefore, if $C\subset\mathcal{A}$,
$$
  \mathbb{P}(x \in C | x\in\mathcal{A}) = \frac{\int_C f_x(x)d\mu(x)}{\int_{\mathcal{A}} f_x(x) d\mu(x)}.
$$
\end{remark}

\begin{remark}
Note that definition \ref{def_conditionalManifold} agrees with the ``usual'' conditional probability (remark \ref{remark_usualConditionalProbability}) as a particular case.
\end{remark}

\subsubsection{Parametrization of $\mathcal{A}$}

Assume that the manifold $\mathcal{A}$ can be parametrized with only one coordinate chart $\phi$,
\begin{equation}\label{eq_phi}
 \begin{aligned}
  \phi: \mathcal{U} \subset \mathbb{R}^k &\to     \phi(\mathcal{U}) = \mathcal{A} \\
              u                           &\mapsto \phi(u). \\
 \end{aligned}
\end{equation}

Notice that the Lebesgue measure in the manifold is
$$
  \mu(A) = \int_{\phi^{-1}(A)} \sqrt{|\textup{det}\left(J_{\phi}(u)^TJ_{\phi}(u)\right)|}du,
$$
for all $\mu$-medible sets $A\subset\mathcal{A}$, where $J_{\phi}$ is the Jacobian of the parametrization $\phi$.
 
The measure $\mu$ can be seen as a measure in the parameters, i.e., a measure in $\mathcal{U} \subset \mathbb{R}^k$,
%$$
%  \hat{\mu}(B) = \int_{B} \sqrt{|\textup{det}\left(J_{\phi}(u)^TJ_{\phi}(u)\right)|}du,
%$$
$$
  \hat{\mu}(B) = \mu(\phi(B))
$$
for all $\hat{\mu}$-medible sets $B\subset\mathcal{U}$.

\subsubsection{PDF w.r.t. the Lebesgue measure on the parameters $u$}

The Radon-Nikodym derivative of $\hat{\mu}$ with respect to the Lebesgue measure in $\mathbb{R}^k$ is
\begin{equation}\label{eq_radonNikodyDerivative}
  \frac{d\hat{\mu}}{du} = \sqrt{|\textup{det}\left(J_{\phi}(u)^TJ_{\phi}(u)\right)|},
\end{equation}
and therefore
\begin{equation} \label{eq_PDFonU}
 \begin{aligned}
  \mathbb{P}(x \in C | x\in\mathcal{A}) &= K\int_{\mathcal{C}} f_x(x)d\mu(x) \\
    &= K\int_{\phi^{-1}(C)} f_x(\phi(u))d\hat{\mu}(u) \\
    &= K\int_{\phi^{-1}(C)} f_x(\phi(u))\frac{d\hat{\mu}}{du} du \\
    &= K\int_{\phi^{-1}(C)} f_x(\phi(u)) \sqrt{|\textup{det}\left(J_{\phi}(u)^TJ_{\phi}(u)\right)|} du. \\
 \end{aligned}
\end{equation}
where the normalization constant is
$$
  K = \frac{1}{
    \int_{\mathcal{U}} f_x(\phi(u)) \sqrt{|\textup{det}\left(J_{\phi}(u)^TJ_{\phi}(u)\right)|} du}.
$$

Remember that the aim is to sample according to a PDF $f_x(x)$ conditioned to a manifold $x\in\mathcal{A}$.
%For example, a manifold given by constraints $g(x)=0$.
%Consider that a chart $\phi$ of $\mathcal{A}$ is found (see (\ref{eq_phi})).
%The derivations in (\ref{eq_PDFonU}) give a tool to sample from $\mathcal{A}$ according to $f_x$.
The method is as follows:
\begin{enumerate}
  \item Let
  $$
    f_u(u) = K f_x(\phi(u)) \sqrt{|\textup{det}\left(J_{\phi}(u)^TJ_{\phi}(u)\right)|}
  $$
  be a PDF on the parameters.
  \item Generate samples according to 
  $$
    x = \phi(u), \ u \sim f_u(u) . 
  $$
%  These are samples from $f_x$ conditioned to $x\in\mathcal{A}$.  
%Use, .e.g., the sampling techniques explained in \ref to sample from $f_u$.
\end{enumerate}

\begin{remark}
  The sampling methods explained in section \ref{sec_samplingArbitraryPDF} can be used to sample from $f_u$. Remember that using these techniques the constant $K$ plays no role.
\end{remark}

\begin{example}
Let $\Omega = (-11,11)\times (-2,2)$.
Let $$\mathcal{A}\subset \Omega \subset \mathbb{R}^2$$
be the ellipse given by the image of the following coordinate chart:
$$
  \begin{array}{crcl}
    \phi: & (0,2\pi) & \to     & \phi(0,2\pi) = \mathcal{A} \subset \mathbb{R}^2 \\
          & u        & \mapsto & (x,y) = \phi(u) = (10\cos(u),\sin(u)). \\
  \end{array}
$$
Let $f_{x,y}(x,y) = C$ ($C$ constant) be the PDF of a uniform distribution in $\Omega$. 
Consider the problem of obtaining samples from $f_{x,y}$ conditioning to $(x,y) \in \mathcal{A}$. In other words, the problem of sampling uniformly in the ellipse $\mathcal{A}$
\footnote{I.e., the probability of two segments of the ellipse is the same if and only if the arc length of the two segments is the same. Notice that the arc length is the Lebesgue measure in the ellipse $\mu$.}.
A naive approach is to generate samples according to
$$
  (x,y) = \phi(u), \ u\sim f_u(u) = f_{x,y}(\phi(u)).
$$
Figures \ref{fig_ellipseSamplesNaive} and \ref{fig_ellipseSamplesNaiveHistogram} show that this method does not give a uniform distribution in the ellipse.
The coordinate chart $\phi$ expands and shrinks volumes, and that alters the probability density.
The term derived in (\ref{eq_radonNikodyDerivative}) is needed to compensate it
\footnote{It is the same situation as the change of variables rule and the Jacobian determinant.}.
Following the derivations in this section, a suitable approach would be
$$
 \begin{array}{l}
  (x,y) = \phi(u), \\
 \begin{aligned}
  u\sim f_u(u) &\propto f_{x,y}(\phi(u))\sqrt{|\textup{det}\left(J_{\phi}(u)^TJ_{\phi}(u)\right)|} \\
  &\propto \|\phi'(u)\| \\
  &\propto \sqrt{10^2\sin^2(u)+\cos^2(u)}.\\
 \end{aligned}
 \end{array}
$$
Figures \ref{fig_ellipseTrueSamples} and \ref{fig_ellipseTrueSampleshistogram} show samples obtained by this method.
\end{example}

\begin{figure}[!htbp]
  \centering
    \includegraphics[width=0.99\textwidth]{figuresEllipse/150_naive_uniform_random_samples_ellipse.eps} 
  \caption[Random samples on an ellipse - Naive method]
{
  $150$ samples of $\phi(u) = (10\cos(u),\sin(u))$, 
  $u \sim \textup{Uniform}(0,2\pi)$. The red points represent
  $\phi(\frac{n\pi}{4}), \ n \in \{ 0,1,2,3,4,5,6,7 \}$.
}
\label{fig_ellipseSamplesNaive}
\end{figure}

\begin{figure}[!htbp]
  \centering
    \includegraphics[width=0.99\textwidth]{figuresEllipse/naiveRandom_histogram_ellipse.png} 
  \caption[Random samples histogram on an ellipse - Naive method]
{
%  \scriptsize
  $10^{5}$ samples of $\phi(t)  = (10\cos(t),\sin(t))$,
  $t \sim \textup{Uniform}(0,2\pi)$,  in an histogram
  whose bins on the ellipse have the same arc length 
  ($10^3$ bins).% and $\frac{\textup{ellipse arc length}}{10^3}$ arc length each bin).
}
\label{fig_ellipseSamplesNaiveHistogram}
\end{figure}

\begin{figure}[!htbp]
  \centering
    \includegraphics[width=0.99\textwidth]{figuresEllipse/150_uniform_random_samples_ellipse.eps} 
  \caption[Uniform random samples on an ellipse]
{
  $150$ samples of $\phi(u) = (10\cos(u),\sin(u))$, 
  $u \sim f_u(u) \propto \sqrt{|\textup{det}(J_{\phi}^T(u)J_{\phi})(u)|} = \|\phi'(u)\|$.
  The red points represent
  $\phi(\frac{n\pi}{4}), \ n \in \{ 0,1,2,3,4,5,6,7 \}$.
}
\label{fig_ellipseTrueSamples}
\end{figure}

\begin{figure}[!htbp]
  \centering
    \includegraphics[width=0.99\textwidth]{figuresEllipse/realRandom_histogram.png} 
  \caption[Uniform random samples histogram on an ellipse]
{
%  \scriptsize
  $10^{5}$ samples of $\phi(t)  = (10\cos(t),\sin(t))$,
  $u \sim f_u(u) \propto \sqrt{|\textup{det}(J_{\phi}^T(u)J_{\phi})(u)|} = \|\phi'(u)\|$,
  in an histogram
  whose bins on the ellipse have the same arc length 
  ($10^3$ bins).% and $\frac{\textup{ellipse arc length}}{10^3}$ arc length each bin).
}
\label{fig_ellipseTrueSampleshistogram}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section[A probabilistic framework for robust inverse design]{A probabilistic framework for robust inverse design under uncertainty} \label{sec_inverseDesign}

The problem of building a stochastic surrogate model from data\footnote{For example, data from computationally expensive simulations.} has been addressed in Chapter \ref{chapter_forwardProblem}. 
It focused in GP and the mathematical foundations of learning theory.

The main novelty of this thesis is given in this section with the development of a framework for design optimization. 
Its aim is to tackle the inverse problem of finding a probability distribution in the input space of a surrogate that satisfies a prescribed performance in the output when uncertainties are propagated.

%This section will present a framework to tackle the inverse problem of finding a probability distribution in the input space of a surrogate that satisfies a prescribed performance in the output when uncertainties are propagated.

The aim is to provide a framework that can be used with any surrogate model. 
Therefore, no assumptions will be taken in the model. 
It will be considered as a black-box which generates an output given an input and no more information is known about its properties.

The notation $\mathcal{X}$ will be used to refer to the space of inputs and the notation $\mathcal{Y}$ will be used to refer to the space of outputs.

Either it is deterministic or stochastic, the only assumption taken on the surrogate model is that given samples $\{x_1, \dots, x_n\}$ from a particular probability distribution on $\mathcal{X}$, it can propagate uncertainties generating samples $\{y_1, \dots, y_n\}$ in $\mathcal{Y}$. See the following diagram:

$$
x \in \mathcal{X} \rightarrow \boxed{ \substack{SURROGATE \\ MODEL} } \rightarrow y(x) \in \mathcal{Y}
$$

The notation $\pmb{x} = \{x_1, \dots, x_n\}$ will be used to refer to a set of samples in the input space of a surrogate model.
The notation $\pmb{y} = \{y_1, \dots, y_n\}$ will be used to refer to a set of samples in the output space.
The notation $\pmb{y}(\pmb{x}) = \{y(x_1), \dots, y(x_n)\}$ will be used to denote that $\pmb{y}(\pmb{x})$ are samples in the output space propagated from the set of samples $\pmb{x}$ through the surrogate model.

\subsection{The general framework}

A parametric family of probability density functions $\mathcal{F}$ indexed by a parameter $\lambda \in S(\lambda)$ is set in $\mathcal{X}$,

$$
  \mathcal{F} = \{f_{\lambda} \ | \ \lambda \in S(\lambda)\}.
$$

Consider a function,
$$
  \begin{aligned}
    \psi : \mathcal{Y} \times S(\lambda) &\to \mathbb{R} \\
                          (y,\lambda)    &\mapsto \psi(y,\lambda),\\
  \end{aligned}
$$
and the optimization problem of finding,
%\begin{equation} \label{eq_optimizationExpectation}
%  \lambda_{\textup{opt}} = \underset{\lambda\in S(\lambda)}{\textup{argmin}}\left\|\mathbb{E}_{x \sim f_{\lambda}}\left[ \psi(y(x),\lambda) \right]\right\|_{W},
%\end{equation}
\begin{equation} \label{eq_optimizationExpectation}
  \lambda_{\textup{opt}} = \underset{\lambda\in S(\lambda)}{\textup{argmin}} \ \ \mathbb{E}_{x \sim f_{\lambda}}\left[ \psi(y(x),\lambda) \right].
\end{equation}
%for some positive definite matrix $W \in \mathbb{R}^{k \times k}$ and associated norm $\|v\|^2_W = v^TWv$.

\begin{remark}
The function $\psi$ must be defined according to the target performance.
\end{remark}

\begin{remark} \label{remark_parametricModel}
The problem of finding a distribution in the input space $\mathcal{X}$ that optimizes the expectation of $\psi$ is reduced to the problem of finding the parameters $\lambda$ which lead to the optimal distribution within the family $\mathcal{F}$.
An important assumption has been taken with this parametric approach.
It may well be that no member of the family $\mathcal{F}$ gives a satisfactory performance. Therefore, the choice of $\mathcal{F}$ is decisive.
\end{remark}

For instance, consider
$$
 \begin{aligned}
  \psi(y(x),\lambda) &= \psi_y(y(x)) + \gamma\psi_{\lambda}(\lambda), \\ 
  \mathbb{E}_{x \sim f_{\lambda}}\left[ \psi(y(x),\lambda) \right] &= 
  \mathbb{E}_{x \sim f_{\lambda}}\left[ \psi_y(y(x)) \right] +
  \gamma\psi_{\lambda}(\lambda), 
 \end{aligned}
$$
for some constant $\gamma \in \mathbb{R}^+$, and the two following examples.

\begin{example} \label{ex_basicFramework1}
Let the surrogate model $y(x) = (y_1(x), y_2(x))$ be two quantities of interest which are wanted to be minimized and are given by the design variables $x \in \mathcal{X} = \mathbb{R}^n$. 
In addition, it is required to have as much flexibility as possible in the design.
The parametric family of densities $\mathcal{F}$ is decided to be the family of $n$-dimensional multivariate normal densities indexed by the mean and the covariance matrix, i.e., $\lambda = (\mu, \Sigma)$.
Let
$$
  \psi_y(y(x)) = \|y(x)\|, \ \ \psi_{\lambda}(\mu,\Sigma) = \frac{1}{\textup{det}(\Sigma)}.
$$
%for some constants $a_1, a_2 \in \mathbb{R}^+$. 
The optimization problem becomes,
$$
 \begin{aligned}
  (\mu_{opt},\Sigma_{opt}) &= \underset{\mu, \Sigma}{\textup{argmin}} \ \ \mathbb{E}_{x \sim \mathcal{N}(\mu, \Sigma)}\left[ \psi(y(x),\lambda) \right] \\
   &= \underset{\mu, \Sigma}{\textup{argmin}} \ \
      \mathbb{E}_{x \sim \mathcal{N}(\mu, \Sigma)}\left[ \|y(x)\| \right] +
      \frac{\gamma}{\textup{det}(\Sigma)}. \\
 \end{aligned}
$$
The norm $\|y(x)\|$ can be given by specific features of the design problem, e.g., 
$$
  \|y(x)\|_{W} = y(x)^T W y(x) 
$$ 
for some positive\hyp{}definite matrix $W$.
The flexibility on the design variables $x$ can be measured by another quantity derived from the covariance matrix $\Sigma$, e.g., the lowest eigenvalue. 
\end{example}

\begin{example} \label{ex_basicFramework2}
Consider the same situation than example \ref{ex_basicFramework1}. However, instead of minimizing $\|y(x)\|$, the quantities given by $y(x)$ are wanted to be (with high probability) in an specific region $A \subset \mathcal{Y}$, which is considered a region of ``good'' performance.
Consider,
$$
  \psi_y(y(x)) = \mathbb{1}_{\mathcal{Y} \setminus A}(y(x)) = \left\{ 
    \begin{array}{l}
      1 \ \text{ if } \ y(x) \in \mathcal{Y} \setminus A, \\
      0 \ \text{ otherwise.}
    \end{array} \right.
$$
Thus,
$$
 \begin{aligned}
  (\mu_{opt},\Sigma_{opt}) &= \underset{\mu, \Sigma}{\textup{argmin}} \ \ \mathbb{E}_{x \sim \mathcal{N}(\mu, \Sigma)}\left[ \psi(y(x),\lambda) \right] \\
   &= \underset{\mu, \Sigma}{\textup{argmin}} \ \
      \mathbb{E}_{x \sim \mathcal{N}(\mu, \Sigma)}\left[ \mathbb{1}_{\mathcal{Y} \setminus A}(y(x)) \right] +
      \frac{\gamma}{\textup{det}(\Sigma)}. \\
   &= \underset{\mu, \Sigma}{\textup{argmin}} \ \
      \mathbb{P}_{x \sim \mathcal{N}(\mu, \Sigma)}\left( y(x) \notin  A \right) +
      \frac{\gamma}{\textup{det}(\Sigma)}. \\
 \end{aligned}
$$
\end{example}

The main problem of tackling (\ref{eq_optimizationExpectation}) is that no information about the surrogate model is given. 
However, it is possible to propagate uncertainties with point evaluations.
The expectation will be approximated with some sample statistic from a sample $\pmb{y}(\pmb{x})$ where,
$$
  \begin{array}{c}
    \pmb{x} = \{x_1, \dots, x_n\}, \\
    x_i \ \text{ i.i.d. , } \ \ x_i \sim f_{\lambda}, \ \ \forall \ 1\leq i \leq n. \\  
  \end{array}
$$
The notation $\pmb{x}_{\lambda} = \{x_{\lambda,1}, \dots, x_{\lambda,n}\}$ will be used to denote that the samples have been generated from $f_{\lambda}$.
In the same fashion, 
$$
  \pmb{y}_{\lambda} = \{y_{\lambda,1}, \dots, y_{\lambda,n}\} = \pmb{y}(\pmb{x}_{\lambda}).
$$
For instance, the expectation in example \ref{ex_basicFramework1} can be approximated by,
$$
  \mathbb{E}_{x \sim \mathcal{N}(\mu, \Sigma)}\left[ \|y(x)\| \right] \approx 
  \frac{1}{n}\sum_{i=1}^n \|y_{\mu,\Sigma,i}\|,
$$
and, in example \ref{ex_basicFramework2} by,
$$
      \mathbb{E}_{x \sim \mathcal{N}(\mu, \Sigma)}\left[ \mathbb{1}_{\mathcal{Y} \setminus A}(y(x)) \right] = 
      \mathbb{P}_{x \sim \mathcal{N}(\mu, \Sigma)}\left( y(x) \notin  A \right) \approx
      \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{\mathcal{Y} \setminus A}(y_{\mu,\Sigma,i}).
$$
Since it has been assumed that the problem can only be tackled with samples, the objective function in the optimization problem will be redefined using the samples as input variables,
$$
 \begin{aligned}
  H: \mathcal{Y}^n \times S(\lambda) &\to \mathbb{R} \\
     (y_1, \dots, y_n, \lambda) = (\pmb{y}, \lambda) &\mapsto H(\pmb{y}, \lambda), \\
 \end{aligned}
$$
and therefore,
\begin{equation} \label{eq_optimizationObjectiveFunctionH}
  \widehat{\lambda}_{\textup{opt}} = \underset{\lambda\in S(\lambda)}{\textup{argmin}} \ \ H(\pmb{y}_{\lambda}, \lambda).
\end{equation}

%The objective function $H$ must be defined according to the desired performance in each particular case.
Estimators can be used to define $H$. For instance, in example \ref{ex_basicFramework1},
\begin{equation} \label{eq_Hexample1}
% \begin{aligned}
  H(\pmb{y},\mu,\Sigma) = \frac{1}{n}\sum_{i=1}^n \|y_{i}\| + 
      \frac{\gamma}{\textup{det}(\Sigma)}, \\
% \end{aligned}
\end{equation}
and in example \ref{ex_basicFramework2},
\begin{equation} \label{eq_Hexample2}
  H(\pmb{y},\mu,\Sigma) =  
      \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{\mathcal{Y} \setminus A}(y(x_i)) +
      \frac{\gamma}{\textup{det}(\Sigma)}. \\
\end{equation}
%\begin{equation}
%  \widehat{\lambda}_{\textup{opt}} = \underset{\mu,\Sigma}{\textup{argmin}} \ \ \frac{a_1}{n}\sum_{i=1}^n \|y_{\mu,\Sigma,i}\| + 
%      \frac{a_2}{\textup{det}(\Sigma)}.
%\end{equation}

A choice of a parametric family $\mathcal{F}$ and an objective function $H$ uniquely determine the optimization problem (\ref{eq_optimizationObjectiveFunctionH}). 
Function $H$ becomes the object of interest and it must be defined according to the desired performance in each particular case.
Although the notation $\widehat{\lambda}_{\textup{opt}}$ was used in (\ref{eq_optimizationObjectiveFunctionH}) to differentiate from (\ref{eq_optimizationExpectation}), the notation without hat will always refer to the solution of (\ref{eq_optimizationObjectiveFunctionH}) in the following sections.

The following sections will give methods to tackle (\ref{eq_optimizationObjectiveFunctionH}) (see section \ref{sec_StochasticOptimization}), potential objective functions $H$ when the aim is to approximate a target PDF in $\mathcal{Y}$ (see section \ref{sec_targetPDFapproximation}), and finally, two potential parametric families of distributions $\mathcal{F}$ (see section \ref{sec_parametricFamilies}).

\begin{remark} \label{remark_HonlyDependsOnLambda}
Notice that the objective function $H$ in (\ref{eq_optimizationObjectiveFunctionH}), in fact, only depends on $\lambda$ since $y_{\lambda}$ depends on $\lambda$. 
However, the samples $\pmb{y}_{\lambda}$ are not uniquely determined since they are generated by a random process. 
Remark \ref{remark_optimizationWithRandom} gives more details about how this issue affects the optimization problem (\ref{eq_optimizationObjectiveFunctionH}).
\end{remark}

\begin{remark} \label{remark_optimizationWithRandom}
Notice that the optimal value $\widehat{\lambda}_{\textup{opt}}$ in the optimization problem \ref{eq_optimizationObjectiveFunctionH} is not well defined. 
The objective function $H$ can be chosen to ensure the existence of a global minimum in some point $(\pmb{y},\lambda)$. 
However, the samples $\pmb{y}_{\lambda}$ depend on $\lambda$ and on the surrogate, and they are not uniquely determined since it also depends on a random process (sampling from $f_{\lambda}$). 

When uncertainties are propagated through the surrogate, not all information given by the distribution $f_{\lambda}$ is used, only the information given by the samples is propagated. 
The minimum in \ref{eq_optimizationObjectiveFunctionH} can change depending on the samples that are used for each possible distribution $f_{\lambda}$. 
Methods which uniquely determine the samples
\footnote{This concept may be confusing because randomness is intrinsic in the process of sampling. The idea is to give a method that generates the same samples if the same distribution is simulated in different occasions. This choice is the representation of the distribution by samples and will never change during the algorithm.}
 $\pmb{y}_{\lambda}$ for each distribution $f_{\lambda}$ will be given (see remark \ref{remark_SamplingOnceMultivariateNormal}) and it will be assumed that the minimum exists for those choices.
In fact, the aim will not be to find the best value of $\lambda$, finding a ``good value will be considered sufficient.
The optimization algorithms that will be proposed do not ensure finding the minimum even if it exists.
However, they are able to explore the space $S(\lambda)$ efficiently with the objective to find, at least, a satisfactory solution.
\end{remark}

\subsection{Stochastic optimization} \label{sec_StochasticOptimization}

The aim of this section is to give a method to solve the optimization problem \ref{eq_optimizationObjectiveFunctionH}.

On the one hand, the method should satisfy two conditions which are necessary to succeed in the stated minimization problem:
\begin{enumerate}
  \item The method must not require any assumption on the objective function. For example, gradient based methods are not suitable since regularity conditions can not be assumed. This is because no information about the surrogate model is given. %and sampling is also involved in the optimization process.
  \item The injectivity of the model can not be assumed and the possibility of many local minimum should be considered. A method able to escape from a local minimum is necessary. 
\end{enumerate}
In addition, its use has to be feasible in terms of computational cost when $S(\lambda)$ is a high-dimensional space.

On the other hand, guaranteeing the global minimum is not necessary, it will be sufficient to find a satisfactory solution.

Algorithms in stochastic optimization (see, e.g., \textcite{spall2003}; or \textcite{zilinskas2016}), which introduce randomness in the search process, are an option which satisfy the necessary conditions stated above. 
In this document, the algorithm of Simulated Annealing (SA) will be explained (section \ref{sec_SimulatedAnnealing}). 
However, other methods can be valid or even show a better performance than SA. 
There are many options in the field of optimization, and other approaches could be explored.
It also depends on the specific problem. 
SA offers a general solution which is the goal of this work.


\subsubsection{Simulated Annealing} \label{sec_SimulatedAnnealing}

Simulated Annealing (SA) is based in M-H (section \ref{sec_MetropolisHastings}) to find the global minimum of an objective function $F(\lambda), \lambda \in S(\lambda)$ exploring the space $S(\lambda)$.

Algorithm \ref{alg_MH} is used with
\begin{equation} \label{eq_SAenergy}
  f(\lambda) = E(\lambda) = e^{-\frac{F(\lambda)}{T}}.
\end{equation}
The parameter $T$ is usually called temperature and $f(\lambda)$ is called the energy at $\lambda$ (the notation $E(\lambda)$ is commonly used in SA theory).

\begin{remark}
Notice that $e^{-t}$ is strictly monotonically decreasing, and therefore the global minimum of $F(\lambda)$ is the highest mode of $E(\lambda)$.
\end{remark}

The main difference between SA and M-H using (\ref{eq_SAenergy}) is that the parameter $T$ is decreased in each iteration (or every $k$ iterations).
Note that decreasing $T$ also decrease the probability of accepting worse solutions. 
At the beginning, a high value of $T$ leads to a high acceptance ratio. 
This allows the algorithm to escape from a local minimum in the first iterations, and therefore it can explore the whole space $S(\lambda)$ (ideally).
However, temperature $T$ progressively decreases to zero. Thus, in the last iterations, the probability of accepting a worse solution is almost zero converging to the global minimum (ideally).

\begin{remark}
The convergence or transitions are not affected by the continuity or
differentiability of function $F(\lambda)$. Therefore, SA can be used even when regularity conditions cannot be guaranteed. 
\end{remark}

The acceptance criteria can be the same than M-H,
$$
  p(\lambda \rightarrow \lambda' | T) = \min\left( e^{-\frac{F(\lambda')-F(\lambda)}{T}} , 1 \right),
  \ \text(see (\ref{eq_metropolisChoice})),  
$$
considering the symmetry of the proposal densities 
$$
  g(\lambda,\lambda') = g(\lambda',\lambda).
$$
Or another option can be chosen. 
It does not need to satisfy the detailed balance condition since the goal is not simulating a distribution. 
In fact, the acceptance criteria can be deterministic (see \textcite{dueck1990}; and \textcite{franz2001}).

The way $T$ decreases plays an important role in the successful convergence of SA. 
It is called cooling schedule.
Most of the improvements in SA are based on improvements of the cooling schedule, e.g., adaptative SA (see, \textcite{ingber1989}; and \textcite{ingber2000}).
More information of SA can be found in, e.g., \textcite{henderson2003} or \textcite{rao2009}.



\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  \pmb{inputs:} Function $F(\lambda)$ to minimize; proposal distributions $g(\lambda,\cdot)$; maximum number of iterations $n_{max}$; initial point $\lambda_0$; initial temperature $T_0$; cooling schedule; acceptance criteria\;
  $T = T_0$\;
  \For{$i=1,\cdots,n_{max}$}
  {
    \tcp{Sample from $g(\lambda_{i-1},\cdot)$}
    $\lambda' \sim g(\lambda_{i-1},\cdot)$\;
%    \tcp{Sample from a uniform distribution}
%    $u = \textup{ran}(0,1)$\;
%    \tcp{Compute the acceptance ratio}
%    $a = \min\left( \frac{f(y)g(y,x)}{f(x)g(x,y)} , 1 \right)$\;
    \uIf{$\lambda'$ satisfies the acceptance criteria based in $F(\lambda_{i-1})$, $F(\lambda')$ and $T$}
    {
      \tcp{Accept}
      $\lambda_i = \lambda'$\;
    }
    \Else
    {
      \tcp{Reject}
      $\lambda_i = \lambda_{i-1}$\;
    }
    Decrease $T$ according to the cooling schedule\;
  }
  \pmb{output:} $\lambda_{n_{max}}$\;
\caption[Simulated Annealing (SA) algorithm]
{
  Simulated Annealing (SA) algorithm. Stochastic optimization method to find the global minimum of a function $F(\lambda)$.
}
\label{alg_SA}
\end{algorithm}
\vspace{5mm}


\subsection{Target PDF approximation} \label{sec_targetPDFapproximation}

The objective function $H$ in the optimization problem (\ref{eq_optimizationObjectiveFunctionH}) must be defined according to the desired performance. For instance, two possible options (\ref{eq_Hexample1}) and (\ref{eq_Hexample2}) were given for two different scenarios (examples \ref{ex_basicFramework1} and \ref{ex_basicFramework2}).

This section will cover in detail another scenario: when the desired performance is given by a target density in the output space $\mathcal{Y}$ of the surrogate model.

Let $f_T(y), y \in \mathcal{Y}$ be a target PDF in the output space of a surrogate model. The aim is to find $\lambda_{opt} \in S(\lambda)$ such that
$$
  x \sim f_{\lambda_{\textup{opt}}} \Rightarrow y(x) \sim f_T,
$$ 
or, at least, the PDF of $y(x)$ is as close 
\footnote{It will be shown that it is possible to set a distance between distributions. See section \ref{sec_distanceDistributions}.}
as possible to $f_T$.% to the target PDF $f_T$.

\subsubsection{A distance between probability distributions} \label{sec_distanceDistributions}

Let $(\mathbb{P},\mathbb{Q}) \mapsto d(\mathbb{P},\mathbb{Q})$ be a distance between probability distributions.
This distance should be able to be estimated from samples since this is the only information available of the distribution of $y(x)$.

Let $\pmb{y} \mapsto \widehat{d}(\pmb{y},\mathbb{P}_{f_T})$ be an estimator of the distance $d$ between the distribution which generated $\pmb{y}$ and the distribution given by $f_T$.

Consider the objective function,
$$
  H(\pmb{y},\lambda) = \widehat{d}(\pmb{y}_{\lambda},\mathbb{P}_{f_T}) + \gamma H_{\lambda}(\lambda).
$$ 

Following this approach, it is necessary to find a distance between distributions that can be estimated from samples. If not a distance, at least, a function able to measure the ``closeness'' of two probability distributions.

Traditional goodness-of-fit tests can be explored for this purpose. 
However, they can become computationally intractable in high dimensions.
It is needed to run the test in each iteration on the optimization process (see section \ref{sec_StochasticOptimization}) when the objective function is evaluated.
Therefore, the computational cost of the test determines if the exploration of $S(\lambda)$ is feasible.

Another important aspect is that goodness-of-fit tests are designed to fit a distribution into a sample and not the opposite
\footnote{For example, maximizing $f_T$ will push the design to concentrate as much as possible the output density at the mode of $f_T$. Although it can be a desired behavior, this is not what it is wanted in this section. The aim is to approximate $f_T$, not only to concentrate the density around the mode.}.

Novel methods arose in the Machine Learning community that can solve some of those problems. 
In this work, it was decided to use Maximum Mean Discrepancy (MMD). 
Probability distributions become points in an RKHS and MMD is the distance given by the inner product in this Hilbert space. 

\subsubsection{Maximum Mean Discrepancy}

MMD is the distance of a Hilbert space known as ``kernel mean embedding'' in the ML community.
Probability distributions are mapped into an RKHS $\mathcal{H}$ (the kernel mean embedding) through the following operator,
$$
  \phi(\mathbb{P}) = \mu_{\mathbb{P}} = \int_{\mathcal{Y}} K(y,\cdot) d\mathbb{P}(y),
$$
where $K: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$ is a PDS kernel (see remark \ref{remark_PDSkernel}). 

\begin{remark}In contrast to Chapter \ref{chapter_forwardProblem} where kernels were used in the input space of the surrogate model, the notation $\mathcal{Y}$ is used in this section to emphasize that the kernel mean embedding is applied to distributions in the output space.
\end{remark}

It can be proved that the map $\mathbb{P} \mapsto \mu_{\mathbb{P}}$ is injective, and therefore
$$
  \|\mu_{\mathbb{P}} - \mu_{\mathbb{Q}}\|_{\mathcal{H}} = 0 \ \text{ if and only if } \ \mathbb{P} = \mathbb{Q}.
$$

\begin{definition}
  Define the MMD as the distance in $\mathcal{H}$ between the mean embedding of two probability distributions $\mathbb{P}$ and $\mathbb{Q}$,
$$
 \begin{aligned}
  MMD&[\mathcal{H},\mathbb{P},\mathbb{Q}] = \|\mu_{\mathbb{P}}-\mu_{\mathbb{Q}}\|_{\mathcal{H}}. \\
 \end{aligned}
$$
\end{definition}

It can be shown that,
$$
  \langle \mu_{\mathbb{P}}, \mu_{\mathbb{Q}} \rangle_{\mathcal{H}} = 
     \langle \mathbb{E}_{y\sim \mathbb{P}}\left[ K(y,\cdot) \right] , 
             \mathbb{E}_{y\sim \mathbb{Q}}\left[ K(y,\cdot) \right] \rangle =
     \mathbb{E}_{\substack{y\sim \mathbb{P} \\ y'\sim \mathbb{Q}}} \left[ K(y,y') \right].
$$
Thus,
$$
 \begin{aligned}
  MMD^2&[\mathcal{H},\mathbb{P},\mathbb{Q}] = \|\mu_{\mathbb{P}}-\mu_{\mathbb{Q}}\|^2_{\mathcal{H}} \\
   &= \langle \mu_{\mathbb{P}}, \mu_{\mathbb{P}} \rangle_{\mathcal{H}}
   -2 \langle \mu_{\mathbb{P}}, \mu_{\mathbb{Q}} \rangle_{\mathcal{H}}
   + \langle \mu_{\mathbb{Q}}, \mu_{\mathbb{Q}} \rangle_{\mathcal{H}} \\
    &= \mathbb{E}_{\substack{y\sim \mathbb{P} \\ y'\sim \mathbb{P}}}   \left[ K(y,y') \right]
    -2\mathbb{E}_{\substack{y\sim \mathbb{P} \\ y\sim \mathbb{Q}}} \left[ K(y,y) \right]
    +\mathbb{E}_{\substack{y\sim \mathbb{Q} \\ y'\sim \mathbb{Q}}} \left[ K(y,y') \right].
 \end{aligned}
$$

More details about these derivations and the theory of kernel mean embedding of distributions can be found in, e.g., \cite{muandet2017}; or \cite{song2008}.

If 
$$
 \begin{array}{c}
  \pmb{y}_T = \{y_{T,1}, \dots, y_{T,m}\} \\
  y_{T,i} \sim f_T, \ 1 \leq i \leq m,
 \end{array}
$$
are i.i.d. samples from the target PDF, and
$$
 \begin{array}{c}
  \pmb{y}(\pmb{x}_{\lambda}) = \{y_{\lambda,1}, \dots, y_{\lambda,n}\} \\
  \pmb{x}_{\lambda} = \{x_{\lambda,1}, \dots, x_{\lambda,n}\}, \ x_{\lambda,i} \sim f_\lambda, \ 1 \leq i \leq n,
 \end{array} 
$$
are i.i.d. samples from a distribution in the input space propagated through the surrogate, then consider,
$$
  H(\pmb{y},\lambda) = \widehat{MMD}^2(\mathcal{H},\pmb{y}(\pmb{x}_{\lambda}),\pmb{y}_T) + \gamma H_{\lambda}(\lambda).
$$ 
where,
\begin{equation} \label{eq_MMDinH}
 \begin{aligned}
  \widehat{MMD}^2(\mathcal{H},\pmb{y}(\pmb{x}_{\lambda}),\pmb{y}_T) = 
         &\frac{1}{m(m-1)} \sum_{i=1}^n \sum_{\substack{j=1 \\ j \neq i}}^n K(y_{\lambda,1},y_{\lambda,j}) \\
        &+ \frac{1}{n(n-1)} \sum_{i=1}^m \sum_{\substack{j=1 \\ j \neq i}}^m K(y_{T,i},y_{T,j}) \\
        &- \frac{2}{mn} \sum_{i=1}^n \sum_{j=1}^m K((y_{\lambda,1},y_{T,j}).  
 \end{aligned}
\end{equation} 
is an estimator of $MMD^2$.

\begin{remark}
  When implementing (\ref{eq_MMDinH}) in SA or another optimization algorithm, note that the double summation on $m$ needs to be computed only once.
\end{remark}

\begin{remark}
  Assuming $m = n$, the complexity of (\ref{eq_MMDinH}) is $\mathcal{O}(n^2)$. SA (or another optimization algorithm) may not be able to explore the space $S(\lambda)$ if $n ,m\gg 0$.
\end{remark}

\subsection{Parametric families of probability distributions} \label{sec_parametricFamilies}

This section will give two options of families $\mathcal{F}$.

The two main restrictions in the selection of $\mathcal{F}$ are:
\begin{enumerate}  
  \item The distributions in $\mathcal{F}$ should have support equal to the input space $\mathcal{X}$.
  \item Generating samples from the distributions in $\mathcal{F}$ should be computationally efficient. 
\end{enumerate}

\begin{remark} \label{remark_constraintsInDesignVariables}
If there are required constraints in the design variables, i.e., the input variables of the surrogate model, then families of distributions in the constrained space can be created from families of distributions in the ambient space using the theory explained in section \ref{sec_samplingWithVariablesConstraints}.
\end{remark}

\subsubsection{Multivariate normal}

In theory, it should be possible to generate samples from any PDF using the methods explained in section \ref{sec_samplingArbitraryPDF}. 
However, it is necessary to simulate a PDF in each iteration of SA (or any other optimization algorithm). 
In addition, the evaluation of $H$ can already be expensive (see, e.g., MMD (\ref{eq_MMDinH})). 

Algorithm \ref{alg_randomMultivariateNormal}, and algorithms \ref{alg_randomNormal} or \ref{alg_randomNormalImprovement}, make a very efficient method to generate samples from any multivariate normal PDF. 

In section \ref{sec_RKHS}, it was assumed the compactness of $\mathcal{X}$. 
In contrast, the support of a multivariate normal distribution is the whole Euclidean space.
Constraints in $\mathcal{X}$ should not be a major issue using multivariate normal distributions if $\mathcal{F}$ is restricted such that most of the probability density of the normal distributions is concentrated on $\mathcal{X}$. 
For example, allowing only normal PDFs with the mean belonging to $\mathcal{X}$. 
An option to avoid possible evaluation problems in the optimization algorithm is to discard densities that generated a sample outside $\mathcal{X}$.
In any case, if it is possible, it is recommended that the surrogate model can evaluate any point in the Euclidean space to avoid this issue.

Let $\mathcal{X} = \mathbb{R}^N$. The multivariate normal densities have two parameters: the mean and the covariance matrix (see section \ref{sec_multivariateNormal}),
$$
  \lambda = (\mu, \Sigma), \ \mu \in \mathbb{R}^N, \ \Sigma \in \mathbb{R}^{N\times N} \ \text{symmetric positive\hyp{}definite.}
$$

In propositions \ref{prop_cholesky} and \ref{prop_SigmaSPD}, it was proved that the covariance matrix of any non-degenerate normal distribution can be decomposed by Cholesky factorization,
$$
  \Sigma \ \substack{\text{symmetric and}\\ \text{positive\hyp{}definite}} \Rightarrow \Sigma = LL^T 
  \left\{ \begin{array}{l}
    \text{ \small $L$ unique, } \\
    \text{ \small $L$ lower triangular ($l_{ij} = 0$ if $i>j$),} \\
    \text{ \small positive diagonal entries ($l_{ii} > 0$).}
  \end{array} \right.
$$
The implication to the left is also true, notice that,
$$
 \begin{array}{c}
  v^TLL^Tv = (L^Tv)^T(L^Tv) > 0, \ \forall v\in\mathbb{R}^N\setminus\{0\}, \ \text{ and } \\
  (LL^T)^T = LL^T.
 \end{array}
$$
Therefore, it can be considered
\begin{equation} \label{eq_lambdaToMuSigma}
 \begin{array}{c}
  \lambda = ( 
        \underbrace{\mu_1,\dots,\mu_N}_{N \text{ elements}},
        \underbrace{l_{21},\dots,l_{N,N-1}}_{\substack{l_{ij}, \ i>j, \\ \text{ $\frac{N^2-N}{N}$ elements}}},
        \underbrace{l_{11},\dots,l_{NN}}_{N \text{ elements}}
        ) \in \mathbb{R}^{\frac{N^2+N}{2}} \times (\mathbb{R}^+\setminus\{0\})^N \\
  \mu = (\mu_1, \dots, \mu_N), \ L = (l_{ij})_{\substack{1\leq i\leq N \\ 1\leq j\leq N}}, \ \Sigma = LL^T.
 \end{array}
\end{equation}
instead.

Algorithm \ref{alg_multivariateNormalOptimizationWithSA} gives the pseudocode to tackle the design problem of this section using multivariate normal densities as $\mathcal{F}$ and SA to optimize its parameters.

%\vspace{5mm}
\begin{algorithm}%[H]
\SetAlgoLined
  \pmb{inputs:} Surrogate model $\pmb{y}(\pmb{x})$; Objective function to minimize: $H(\pmb{y}, \lambda), \ \lambda\in\mathbb{R}^{\frac{N^2+N}{2}} \times (\mathbb{R}^+\setminus\{0\})^N$; Proposal distributions $g(\lambda,\cdot)$; Maximum number of iterations $n_{max}$; Initial point $\lambda_0$; Initial temperature $T_0$; Cooling schedule; Acceptance criteria\;
  \tcp{In this pseudocode, $\mu$ and $L$ are obtained from $\lambda$. See (\ref{eq_lambdaToMuSigma}).}
  $T = T_0$\;
  \tcp{Generate samples from $\mathcal{N}(0,I_{N\times N})$. See Algorithms \ref{alg_randomNormal} and \ref{alg_randomNormalImprovement}.}
  $\pmb{x}_s = \{x_{s,1},\dots,x_{s,n}\}, \ x_{s,i}\in \mathbb{R}^{N}, \ x_{s,i} \sim \mathcal{N}(0,I_{N\times N})$  i.i.d.\;
  \tcp{Transform to samples from $\mathcal{N}(\mu_0,L_0L_0^T)$. See section \ref{sec_multivariateNormal}.}
  \For{$i=1,\cdots,n$}
  {
    \tcp{$\pmb{x}_0 = \{x_{0,1},\dots,x_{0,n}\}$}
    $x_{0,i} = \mu_0 + L_0x_{s,i}$\;
  }
  \tcp{Evaluate in the surrogate model}
  $\pmb{y}_0 = \pmb{y}(\pmb{x}_0)$\;
  \For{$i=1,\cdots,n_{max}$}
  {
    \tcp{Sample from $g(\lambda_{i-1},\cdot)$}
    $\lambda_p \sim g(\lambda_{i-1},\cdot)$\;
    \For{$i=1,\cdots,n$}
    {
      \tcp{$\pmb{x}_p = \{x_{p,1},\dots,x_{p,n}\}$}
      $x_{p,i} = \mu_p + L_px_{s,i}$\;
    }
    $\pmb{y}_p = \pmb{y}(\pmb{x}_p)$\;
    \uIf{$\lambda_p$ satisfies the acceptance criteria based in $H(\pmb{y}_{i-1},\lambda_{i-1})$, $H(\pmb{y}_p,\lambda_p)$ and $T$}
    {
      \tcp{Accept}
      $\lambda_i = \lambda_p$\;
    }
    \Else
    {
      \tcp{Reject}
      $\lambda_i = \lambda_{i-1}$\;
    }
    Decrease $T$ according to the cooling schedule\;
  }
  \pmb{output:} $\lambda_{n_{max}}$\;
\caption[Optimizing the parameters of normal PDFs with SA]
{
  Algorithm to optimize the parameters of normal PDFs with SA. See section \ref{sec_SimulatedAnnealing} for more details about SA.
}
\label{alg_multivariateNormalOptimizationWithSA}
\end{algorithm}
%\vspace{5mm}

\begin{remark} \label{remark_SamplingOnceMultivariateNormal}
Notice that the process of generating samples is done only once, from $\mathcal{N}(0,I_{N\times N})$. 
There are no more uncertainties when the SA optimization starts.
Therefore, the samples that are obtained for each choice of $\lambda$ are uniquely determined before the SA process starts. See remark \ref{remark_optimizationWithRandom}.
\end{remark}

\subsubsection{Mixtures} \label{sec_mixtures}

Let $\mathcal{F}_{\alpha}$ be a parametric family of probability distributions with parameter $\alpha$ with support $\mathcal{X}$. Consider the family of mixture densities whose $M$ components are members of $\mathcal{F}_c$,
$$
      \mathcal{F} = \left\{ \ f_{\pmb{\alpha},\pmb{\pi}} = \sum_{i=1}^M \pi_i f_{\alpha_i} \ \Bigg\vert \ \begin{array}{l} 
         f_{\alpha_i} \in \mathcal{F}_{\alpha} \\ 
         \pmb{\alpha} = (\alpha_1, \dots, \alpha_M) \in S(\alpha)^M \\ 
         \pmb{\pi} = (\pi_1,\dots,\pi_M) \in \left\{ \sum_{i=1}^M \pi_i=1, \ \pi_i \geq 0 \right\} \ 
 \end{array} \right\},
$$
where $\lambda = (\pmb{\alpha},\pmb{\pi})$.

A mixture model mitigates the problem stated in remark \ref{remark_parametricModel}.
$\mathcal{F}$ is more flexible in order to approximate the true density that optimizes the objective function over the whole space of density functions with support $\mathcal{X}$.
For example, it is well known that a mixture of normal densities can approximate any multivariate density with any degree of accuracy given enough components (see, e.g., \textcite{titterington1985}).
I.e., mixtures of normal densities with a finite number of components is dense in the whole space of density functions.
The more components, the more flexibility.

Therefore, mixtures are interesting from a mathematical point of view.
However, the suitability of mixtures will depend on each particular problem. 
A mixture may not give useful information for engineering design.  

Algorithm \ref{alg_generateSamplesMixture} gives a method to generate samples from a mixture.

\vspace{5mm}
\begin{algorithm}[H]
\SetAlgoLined
  \pmb{inputs:} Weights $\{\pi_1, \dots, \pi_M\}$; \ Components $\{f_{\alpha_i}\}_{1\leq i \leq M}$.
  \tcp{Generate a uniform random number on $(0,1)$. See section \ref{sec_uniformDistribution}.}
  $u = rand()$\;
  $i = 1$\;
  \While{$u>\pi_i$}
  {
    $u = u-\pi_i$\;
    $i = i+1$\;
  }
  \tcp{Generate a sample from the $i$th component.}
  $x \sim f_{\alpha_i}$\;
  \pmb{output:} $x$\;
\caption[Algorithm to generate samples from a mixture PDF]
{
  Algorithm to generate samples from a mixture PDF. It is assumed that it is possible to simulate the components of the mixture.
}
\label{alg_generateSamplesMixture}
\end{algorithm}
\vspace{5mm}

In the same fashion than \ref{alg_multivariateNormalOptimizationWithSA}, algorithm \ref{alg_mixturesOptimizationWithSA} gives the pseudocode of the stated problem using mixture densities.

%\vspace{5mm}
\begin{algorithm}%[H]
\SetAlgoLined
  \pmb{inputs:} Surrogate model $\pmb{y}(\pmb{x})$; Objective function to minimize $H(\pmb{y}, \lambda), \ \lambda = (\pmb{\alpha},\pmb{\pi})$; Proposal distributions $g(\lambda,\cdot)$; Maximum number of iterations $n_{max}$; Initial point $\lambda_0 = (\pmb{\alpha}_0,\pmb{\pi}_0)$; Initial temperature $T_0$; Cooling schedule; Acceptance criteria; Seed of the pseudorandom number generator $S$\;
  $T = T_0$\;
  \tcp{Generate samples from $f_{\pmb{\alpha}_0,\pmb{\pi}_0}$. See algorithm \ref{alg_generateSamplesMixture}}
  Reset the random number generator with seed equal to $S$\;
  $\pmb{x}_0 = \{x_{0,1},\dots,x_{0,n}\}, \ x_{0,i} \sim f_{\pmb{\alpha}_0,\pmb{\pi}_0}$  i.i.d.\;
  \tcp{Evaluate in the surrogate model}
  $\pmb{y}_0 = \pmb{y}(\pmb{x}_0)$\;
  \For{$i=1,\cdots,n_{max}$}
  {
    \tcp{Sample from $g(\lambda_{i-1},\cdot)$}
    $\lambda_p \sim g(\lambda_{i-1},\cdot)$\;
    \tcp{Generate samples from $f_{\pmb{\alpha}_p,\pmb{\pi}_p}$. See algorithm \ref{alg_generateSamplesMixture}}
    Reset the random number generator with seed equal to $S$\;
    $\pmb{x}_p = \{x_{p,1},\dots,x_{p,n}\}, \ x_{p,i} \sim f_{\pmb{\alpha}_p,\pmb{\pi}_p}$  i.i.d.\;
    $\pmb{y}_p = \pmb{y}(\pmb{x}_p)$\;
    \uIf{$\lambda_p$ satisfies the acceptance criteria based in $H(\pmb{y}_{i-1},\lambda_{i-1})$, $H(\pmb{y}_p,\lambda_p)$ and $T$}
    {
      \tcp{Accept}
      $\lambda_i = \lambda_p$\;
    }
    \Else
    {
      \tcp{Reject}
      $\lambda_i = \lambda_{i-1}$\;
    }
    Decrease $T$ according to the cooling schedule\;
  }
  \pmb{output:} $\lambda_{n_{max}}$\;
\caption[Optimizing the parameters of mixture PDFs with SA]
{
  Algorithm to optimize the parameters of mixture PDFs with SA. See section \ref{sec_SimulatedAnnealing} for more details about SA.
}
\label{alg_mixturesOptimizationWithSA}
\end{algorithm}
%\vspace{5mm}

\begin{remark}
  Notice that the pseudorandom number generator is reset with the same seed just before generating samples in algorithm \ref{alg_mixturesOptimizationWithSA}.
Therefore, given a seed, the samples are uniquely determined. See remark \ref{remark_parametricModel}. 
\end{remark}

\begin{remark}
A method to construct a proposal distribution on the space of weights, 
\begin{equation} \label{eq_mixtureWeightsSpace}
  \left\{ \pmb{\pi} = (\pi_1,\dots,\pi_M) \Bigg\vert \sum_{i=1}^M \pi_i=1, \ \pi_i \geq 0 \right\}
\end{equation}
is to use a proposal distribution $g(\pmb{\pi}_{\textup{previous}},\cdot)$ on $(\mathbb{R}^+)^N$ and normalize,
$$
 \begin{array}{c} 
  \hat{\pmb{\pi}} = (\hat{\pi}_1,\dots,\hat{\pi}_M) \sim g(\pmb{\pi}_{\textup{previous}},\cdot), \\ 
  \pmb{\pi}_{\textup{next}} = \frac{\hat{\pmb{\pi}}}{\sum_{i=1}^M \hat{\pi}_i}.
 \end{array}
$$
\end{remark}

\begin{remark}
  If it is wanted to use a sampling algorithm
\footnote{E.g., M-H or any other MCMC algorithm.} in the space of weights (or any other constrained space) instead of SA, then it is necessary to consider the observations in section \ref{sec_samplingWithVariablesConstraints}. Using a sampling algorithm can be interesting if extracting information from the chain is required
\footnote{The chain in M-H (or other MCMC sampling algorithms) approximates the density. The chain in SA gives no information.}.
\end{remark}

\begin{example} \label{ex_twoModeSurrogate}
This is an example of the design optimization problem stated in this section. 
This example is interesting only from a theoretical point of view.
%Consider a surrogate model given by a deterministic function:
The role of the surrogate model is taken by the following deterministic function,
\begin{equation} \label{eq_surrogateExample1}
%      \mathcal{Y}: 
%      & (0,1)^{2}                     & \to     & \mathbb{R} \\
  y(x_1,x_2) = 
       2\frac{-\exp(-\frac{-v_1^Tv_1}{\sigma^2})-\exp(-\frac{-v_2^Tv_2}{\sigma^2})}{s},
\end{equation}
where
\begin{equation} \label{eq_surrogateExample1extra}
\begin{aligned} 
      v_1 &= (x_1-\frac{1}{3}, \ x_2-\frac{2}{3})^T, \\
      v_2 &= (x_1-\frac{2}{3}, \ x_2-\frac{1}{3})^T, \\
      \sigma^2 &= 0.05, \\
      s &= 1+\exp(\frac{-w^Tw}{\sigma^2}),\\
      w &= (\frac{1}{3}, \ \frac{1}{3})^T, \\
\end{aligned}
\end{equation} 
%    where
%    \begin{itemize}
%    \end{itemize}
%  \end{itemize}  
which has only two inputs and one output.
Consider a displaced log-normal distribution with parameters $\mu=-1.025$ and $\sigma = 0.7644$ and a displacement of $-2.1$, as a target PDF.
It is required to find a PDF in the input space of the surrogate model such that the PDF obtained in the output space, after propagating uncertainties, is as close as possible to the target density. 
Figure \ref{fig_surrogateAndTargetPDF} shows the surrogate model and the target PDF.
%The target PDF mode was set close to the surrogate's minimum (log-normal with parameters $\mu=-1.025$ and $\sigma = 0.7644$ and a displacement of $2.1$).
The low dimensionality was chosen to allow its visualization. % of the surrogate model and the target PDF.
However, it is a difficult scenario because most of the regions in the input space lead to the tail of the target PDF. % when uncertainties are propagated through the surrogate model.
The parametric family $\mathcal{F}$ was a mixture of 3 normal distributions.
Maximum Mean Discrepancy (see (\ref{eq_MMDinH})) was used as the objective function.
%The objective function was exclusively the Maximum Mean Discrepancy with respect to a set of samples from the target PDF.
SA was used to optimize the parameters of the mixture.
Figure \ref{fig_results} shows the results.
Notice that the random initial point led to a very poor approximation.
From this very poor initial point, SA moved to a region of $S(\lambda)$ that led to the desired performance.
Eventually, after the optimization process, the last point given by SA approximates the target PDF satisfactorily.
\end{example}

\begin{figure}[!htbp]
  \centering
%    \includegraphics[width=0.99\textwidth]{twoModeSurrogateExample/3DtwoModeSurrogate.png} 
    \includegraphics[width=0.99\textwidth]{twoModeSurrogateExample/targetPDF.png} 
    \includegraphics[width=0.99\textwidth]{twoModeSurrogateExample/3DtwoModeSurrogateAndTargetPDF.png} 
  \caption[Example \ref{ex_twoModeSurrogate} - surrogate model and target PDF]
{
  Surrogate model and target PDF of example \ref{ex_twoModeSurrogate}.
  \emph{Blue line}: Target PDF. 
  \emph{Surface}: Surrogate model and target PDF. 
}
\label{fig_surrogateAndTargetPDF}
\end{figure}

\begin{figure}[!htbp]
  \centering
%    \includegraphics[width=0.99\textwidth]{twoModeSurrogateExample/3DtwoModeSurrogate.png} 
    \includegraphics[width=0.99\textwidth]{twoModeSurrogateExample/PDFresultsOnlyTheta0.png} 
    \includegraphics[width=0.99\textwidth]{twoModeSurrogateExample/PDFresultsOnlyBestOfChain.png} 
  \caption[Example \ref{ex_twoModeSurrogate} - results]
{
  Results of example \ref{ex_twoModeSurrogate}.
  \emph{Dark blue line}: Target PDF.
  \emph{Red line}: PDF from the random intial point in the SA optimization process. 
  \emph{Black line}: PDF from the last point in the SA optimization process. 
}
\label{fig_results}
\end{figure}

Example \ref{ex_twoModeSurrogate} shows that the algorithm \ref{alg_mixturesOptimizationWithSA} succeeded in a difficult but low dimensional scenario. The following example \ref{ex_twoModeSurrogate10dim} aims to test algorithm \ref{alg_mixturesOptimizationWithSA} for PDF approximation on a bigger scale. The complexity of each iteration should not be affected by the dimensionality of the input space. Notice that the only step that is affected by the dimensionality in each iteration is the sampling process. The sampling complexity grows linearly for normal distributions. Sampling from an $N$-dimensional normal distribution is equivalent to sampling from $N$ unidimensional distributions (see corollary \ref{corollary_nonStandardNormalFromStandardNormal}). 

Example \ref{ex_twoModeSurrogate10dim} will use mixtures of normal distributions in the same fashion as example \ref{ex_twoModeSurrogate}. Theoretically, each iteration should have similar complexity since the sampling grows linearly, and therefore, it is asymptotically irrelevant. Computing the MMD will be the asymptotically relevant operation with regards to computational complexity. Notice that computing the MMD is not affected by the input space dimensionality. 

Although each iteration complexity will not be affected by the input space dimensionality, it may be that the algorithm does not succeed in a high dimensional scenario. Exploring a high dimensional space may need many more iterations before getting a good approximation of the target PDF. The following example uses a 10-dimensional input space and proves that algorithm \ref{alg_mixturesOptimizationWithSA} can explore and get good results in high-dimensional spaces. Notice that $S(\lambda)$ is a $59$-dimensional space if the covariance matrices considered are of the form $\sigma^2 I_{10 \times 10}$ \footnote{$4$ parameters in the simplex mapping for the weights, $50$ parameters for the $5$ means of a $10$-dimensional space and $5$ parameters associated to the covariance matrices of the form $\sigma^2 I_{10 \times 10}$  }. $10^4$ iterations were used in example \ref{ex_twoModeSurrogate}. However, they were not sufficient in a higher-dimensional space. There were needed $10^6$ iterations to get the results shown in the example \ref{ex_twoModeSurrogate10dim}.

\begin{example} \label{ex_twoModeSurrogate10dim}
This example is analogous to the example \ref{ex_twoModeSurrogate}. However, the surrogate's input space will be a 10-dimensional space. Using the same function
$$ 
%      \mathcal{Y}: 
%      & (0,1)^{2}                     & \to     & \mathbb{R} \\
  \hat{y}(x_1,x_2) = 
       2\frac{-\exp(-\frac{-v_1^Tv_1}{\sigma^2})-\exp(-\frac{-v_2^Tv_2}{\sigma^2})}{s}, $$
used in example \ref{ex_twoModeSurrogate} (see (\ref{eq_surrogateExample1}) and (\ref{eq_surrogateExample1extra})), the surrogate model, in this case, will be 
$$
	y(x_1,...,x_{10}) = \sum_{i=1}^{5} \hat{y}(x_{2i-1},x_{2i}).
$$
A normal distribution $\mathcal{N}(-5.5,1)$ is considered as a target PDF.
A mixture of $5$ normal distributions is considered as the parametric family $\mathcal{F}$.
Maximum Mean Discrepancy (see (\ref{eq_MMDinH})) was used as the objective function.
SA was used to optimize the parameters of the mixture.
Figure \ref{fig_results2} shows the target PDF approximation from the random initial point to the last point given by SA.
Notice that the algorithm has been able to approximate the target PDF satisfactorily with conclusions analogous to example \ref{ex_twoModeSurrogate}.
\end{example}

\begin{figure}[!htbp]
  \centering
%    \includegraphics[width=0.99\textwidth]{twoModeSurrogateExample/3DtwoModeSurrogate.png} 
    \includegraphics[width=0.99\textwidth]{twoModeSurrogate2Example/PDFresults2OnlyTheta0.eps} 
    \includegraphics[width=0.99\textwidth]{twoModeSurrogate2Example/PDFresults2OnlyBestOfChain.eps} 
  \caption[Example \ref{ex_twoModeSurrogate10dim} - results]
{
  Results of example \ref{ex_twoModeSurrogate10dim}.
  \emph{Dark blue line}: Target PDF.
  \emph{Red line}: PDF from the random intial point in the SA optimization process. 
  \emph{Black line}: PDF from the last point in the SA optimization process. 
}
\label{fig_results2}
\end{figure}

Example \ref{ex_twoModeSurrogate} and example \ref{ex_twoModeSurrogate10dim} show the potential of algorithm \ref{alg_mixturesOptimizationWithSA}. Example \ref{ex_twoModeSurrogate} proposes a challenging scenario. Most of the regions in the input space lead to the tail of the target PDF. The algorithm has been able to move from a very deficient random initial point to a point in the parameter space $S(\lambda)$ that approximates the target PDF with a small error (see figure \ref{fig_results}). Example \ref{ex_twoModeSurrogate} uses a surrogate model with a two-dimensional input space. However, example \ref{ex_twoModeSurrogate10dim} proves that the algorithm can also give satisfactory results in high-dimensional spaces (see figure \ref{fig_results2}).

\subsection{Gradient-based optimization} \label{sec_gradientBasedOptimization}


Remember that the aim is to optimize the parameters $\lambda$ of the parametric family $\mathcal{F}$. 
It has been used the terminology ``objective function'' to refer to $H(\pmb{y}, \lambda)$ during this chapter because  $\pmb{y}$ depends on $\lambda$ in the optimization process (see remark \ref{remark_HonlyDependsOnLambda}).
Thus, to be rigorous, a function $G(\lambda) = H(\pmb{y}_{\lambda},\lambda)$ should be defined.


Let $\mathcal{F}$ be the parametric family of $N$-dimensional normal distributions.
Let 
$$
  \lambda \in \mathbb{R}^{\frac{N^2+N}{2}} \times (\mathbb{R}^+\setminus\{0\})^N
$$ 
be the parameter of this family.
Let $\mu_{\lambda}$ and $L_{\lambda}$ be the mean and the lower triangular matrix associated to $\lambda$
\footnote{The matrix $\Sigma_{\lambda} = L_{\lambda}L_{\lambda}^T$ is the covariance matrix associated to $\lambda$.}
(see (\ref{eq_lambdaToMuSigma})).
Let $\pmb{x}_s = \{x_{s,1} \dots x_{s,n}\}$ be a set of samples from the standard $N$-dimensional normal distribution, $\mathcal{N}(0,I_{N \times N})$.
Using the same strategy as algorithm $\ref{alg_multivariateNormalOptimizationWithSA}$, the idea is to generate samples only once, before the optimization process starts. 
%Let
%$$
% \begin{aligned}
%  R_{x_i}: \mathbb{R}^{\frac{N^2+N}{2}} &\to \mathbb{R}^N \\
%                               \lambda  &\mapsto \mu_{\lambda} + L_{\lambda}x_{s,i}. \\
% \end{aligned}
%$$

In this Chapter, it was assumed that information about the surrogate model is not available, and therefore the regularity of $G$ is not guaranteed. However, consider a surrogate model $y(x)$ such that,
\begin{equation} \label{eq_linearFunctionsToSamplingFromNormal}
 \begin{aligned}
  y_{x_{s,i}}: \mathbb{R}^{\frac{N^2+N}{2}} \times (\mathbb{R}^+\setminus\{0\})^N &\to \mathcal{Y} \\
                               \lambda  &\mapsto y(\mu_{\lambda} + L_{\lambda}x_{s,i})
 \end{aligned}
\end{equation}
is differentiable for all $1 \leq i \leq n$.

\begin{example}
For example, let $\mathcal{Y}=\mathbb{R}$ and $\pmb{y}(\pmb{x_*}) = (y_{\pmb{x_*},1}, \dots, y_{\pmb{x_*},n})$ be the mean of a GP surrogate (see (\ref{eq_GPconditionalForRegression})) for a set of test points $\pmb{x_*} = \{x_{*1},\dots,x_{*n}\}$.
The notation $\pmb{x_*}$ from Chapter \ref{chapter_forwardProblem} was retrieved to avoid confusion with the training data notation $\pmb{x}$ in the GP.
In this case, note that $\lambda \mapsto \pmb{y}(\mu_{\lambda} + L_{\lambda}x_{s,i})$ is differentiable if the kernel is differentiable.
\end{example}

Consider the function $\lambda \mapsto \pmb{y}_{\lambda} = (y_{x_{s,1}}(\lambda), \dots, y_{x_{s,n}}(\lambda))$. I.e., $\pmb{y}_{\lambda}$ are the fix set of points $\{x_{s,1},\dots,x_{s,n}\}$ transformed to be samples according to $\lambda$ and propagated through the surrogate model.
Notice that the transformation and the surrogate evaluation are differentiable with respect to $\lambda$.
Therefore, 
$$
  G(\lambda) = H(\pmb{y}_{\lambda},\lambda) \text{ differentiable, \ if and only if, \ }
  H(\pmb{y},\lambda) \text{ differentiable.}
$$   

Using this approach and a differentiable function $H$ is possible to use gradient-based optimization algorithms which are generally more computationally efficient than the metaheuristics presented in this work (section \ref{sec_StochasticOptimization}).
However, using these algorithms requires special care to avoid getting trap in a local minimum.
%\begin{remark}
%  With this approach, the information given by the covariance of the GP surrogate is not used.
%\end{remark} 


\vspace{10mm}
\noindent
A design optimization problem was stated in this chapter. 
It consisted of optimizing the parameters of a PDF in the input space of a surrogate model in order to satisfy prescribed performance in the outputs.
Section \ref{sec_inverseDesign} introduced a novel framework to tackle this inverse problem.
Section \ref{sec_pseudorandomNumberGenerator} was dedicated to the explanation of different methods to sample from a PDF, which is a fundamental process for this framework.
Special care was taken to define a framework as general as possible.
It can be applied to many scenarios. First, it does not make assumptions on the model. Secondly, it was defined such that different optimization methods can be used (SA is proposed as a general solution for the optimization problem).
Finally, the objective function can be tuned in order to satisfy different requirements.

The following chapter gives guidelines to apply this framework to a specific design problem.



