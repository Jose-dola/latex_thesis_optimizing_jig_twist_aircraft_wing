
@article{alvarez2012,
  title = {Kernels for {{Vector}}-{{Valued Functions}}: A {{Review}}},
  shorttitle = {Kernels for {{Vector}}-{{Valued Functions}}},
  author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  year = {2012},
  month = apr,
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a regularization perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a probabilistic perspective they are the key in the context of Gaussian processes, where the kernel function is known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  archivePrefix = {arXiv},
  eprint = {1106.6251},
  eprinttype = {arxiv},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/JDVI2CSZ/Alvarez et al. - 2012 - Kernels for Vector-Valued Functions a Review.pdf},
  journal = {arXiv:1106.6251 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@book{anderson2003,
  title = {An Introduction to Multivariate Statistical Analysis},
  author = {Anderson, Theodore Wilbur.},
  year = {2003},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken N.J}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/ZVT62H27/An Introduction to Multivariate Statistical Analysis (Wiley Series in Probability and Statistics) - 3rd edition by T. W. Anderson (z-lib.org).djvu},
  isbn = {0-471-36091-0 978-0-471-36091-9},
  language = {English}
}

@incollection{antunes2017,
  title = {A Review of Heteroscedasticity Treatment with Gaussian Processes and Quantile Regression Meta-Models},
  booktitle = {Springer {{Geography}}},
  author = {Antunes, Francisco and O'Sullivan, Aidan and Rodrigues, Filipe and Pereira, Francisco},
  year = {2017},
  month = oct,
  pages = {141--160},
  isbn = {978-3-319-40900-9}
}

@book{aronszajn1950,
  title = {Introduction o the Theory of {{Hilbert}} Spaces},
  author = {Aronszajn, N.},
  year = {1950},
  publisher = {{Research Foundation}},
  address = {{Stillwater [Oklahoma]}},
  language = {English}
}

@article{aronszajn1950a,
  title = {Theory of Reproducing Kernels},
  author = {Aronszajn, N.},
  year = {1950},
  month = mar,
  volume = {68},
  pages = {337--337},
  issn = {0002-9947},
  doi = {10.1090/S0002-9947-1950-0051437-7},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/KZPJEC6T/Aronszajn - 1950 - Theory of reproducing kernels.pdf},
  journal = {Transactions of the American Mathematical Society},
  language = {en},
  number = {3}
}

@article{bell1968,
  title = {Algorithm 334: {{Normal}} Random Deviates},
  author = {Bell, James R.},
  year = {1968},
  month = jul,
  volume = {11},
  pages = {498},
  issn = {0001-0782},
  doi = {10.1145/363397.363547},
  journal = {Communications of the ACM},
  keywords = {frequency distribution,normal deviates,normal distribution,probability distribution,random,random number,random number generator,simulation},
  number = {7}
}

@article{betancourt2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = jul,
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  archivePrefix = {arXiv},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/8QLI7Q5W/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf},
  journal = {arXiv:1701.02434 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{bilionis2013,
  title = {Multi-Output Separable {{Gaussian}} Process: {{Towards}} an Efficient, Fully {{Bayesian}} Paradigm for Uncertainty Quantification},
  author = {Bilionis, Ilias and Zabaras, Nicholas and Konomi, Bledar and Lin, Guang},
  year = {2013},
  month = may,
  volume = {241},
  pages = {212--239},
  doi = {10.1016/j.jcp.2013.01.011},
  journal = {Journal of Computational Physics}
}

@article{blum2001,
  title = {Metaheuristics in Combinatorial Optimization: {{Overview}} and Conceptual Comparison},
  author = {Blum, Christian and Roli, Andrea},
  year = {2001},
  month = jan,
  volume = {35},
  pages = {268--308},
  doi = {10.1145/937503.937505},
  journal = {ACM Computing Surveys}
}

@book{bobrowski2005,
  title = {Functional Analysis for Probability and Stochastic Processes : An Introduction},
  author = {Bobrowski, Adam.},
  year = {2005},
  publisher = {{Cambridge Univ. Press}},
  address = {{Cambridge}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/LPJB4WZA/2006 - Functional analysis for probability and stochastic.pdf},
  isbn = {0-521-53937-4 0-521-83166-0 978-0-521-83166-6 978-0-521-53937-1},
  language = {English}
}

@article{box1958,
  title = {A {{Note}} on the {{Generation}} of {{Random Normal Deviates}}},
  author = {Box, G. E. P. and Muller, Mervin E.},
  year = {1958},
  month = jun,
  volume = {29},
  pages = {610--611},
  publisher = {{The Institute of Mathematical Statistics}},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177706645},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/IRNCAC9M/Box y Muller - 1958 - A Note on the Generation of Random Normal Deviates.pdf},
  journal = {Ann. Math. Statist.},
  language = {en},
  number = {2}
}

@book{brooks2011,
  title = {Handbook for {{Markov}} Chain {{Monte Carlo}}},
  author = {Brooks, Steve},
  year = {2011},
  publisher = {{Taylor \& Francis}},
  address = {{Boca Raton}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/TJI4MA6Z/Brooks - 2011 - Handbook for Markov chain Monte Carlo.pdf},
  isbn = {978-1-4200-7941-8 1-4200-7941-7},
  language = {English}
}

@incollection{burczynski2000,
  title = {- {{Evolutionary}} Methods in Inverse Problems of Engineering Mechanics},
  booktitle = {Inverse {{Problems}} in {{Engineering Mechanics II}}},
  author = {Burczy{\'n}ski, T. and Beluch, W. and D{\fontencoding{LELA}\selectfont\char202}ugosz, A. and Orantek, P. and Nowakowski, M.},
  editor = {Tanaka, M. and Dulikravich, G. S.},
  year = {2000},
  month = jan,
  pages = {553--562},
  publisher = {{Elsevier Science Ltd}},
  address = {{Oxford}},
  doi = {10.1016/B978-008043693-7/50131-8},
  abstract = {This paper deals with applications of evolutionary algorithms to inverse problems of engineering mechanics. Evolutionary algorithms are considered as modified and generalized classical genetic algorithms in which populations of chromosomes are coded by floating point representation, and the new modified crossover and mutation operations are introduced. The evolutionary algorithm starts with a population of randomly generated chromosomes from a feasible solution domain. These chromosomes, which have the vector structure, evolve toward better solutions by applying genetic operators such as selection, mutation, and crossover. After applying genetic operators, the new population has a better fitness. The probability of crossover and mutation does not have to be constant as in classical genetic algorithms and it can change during the evolutionary process. An objective function (fitness function) with constraints plays the role of the environment to distinguish between good and bad chromosomes and to select the better solution.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/PEZT5JAX/B9780080436937501318.html},
  isbn = {978-0-08-043693-7},
  language = {en}
}

@article{calandra2016,
  title = {Manifold {{Gaussian Processes}} for {{Regression}}},
  author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
  year = {2016},
  month = apr,
  abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and nondifferentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
  archivePrefix = {arXiv},
  eprint = {1402.5876},
  eprinttype = {arxiv},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/C2QJJBG2/Calandra et al. - 2016 - Manifold Gaussian Processes for Regression.pdf},
  journal = {arXiv:1402.5876 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{carmeli2006,
  title = {Vector Valued {{Reproducing Kernel Hilbert Spaces}} of Integrable Functions and {{Mercer}} Theorem},
  author = {Carmeli, Claudio and De Vito, Ernesto and Toigo, Alessandro},
  year = {2006},
  month = oct,
  volume = {04},
  pages = {377--408},
  issn = {0219-5305, 1793-6861},
  doi = {10.1142/S0219530506000838},
  abstract = {We characterize the reproducing kernel Hilbert spaces whose elements are p-integrable functions in terms of the boundedness of the integral operator whose kernel is the reproducing kernel. Moreover, for p = 2, we show that the spectral decomposition of this integral operator gives a complete description of the reproducing kernel, extending the Mercer theorem.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/MXD8935L/Carmeli et al. - 2006 - VECTOR VALUED REPRODUCING KERNEL HILBERT SPACES OF.pdf},
  journal = {Analysis and Applications},
  language = {en},
  number = {04}
}

@article{casella1992,
  title = {Explaining the Gibbs Sampler},
  author = {Casella, George and George, Edward I.},
  year = {1992},
  volume = {46},
  pages = {167--174},
  issn = {00031305},
  abstract = {Computer-intensive algorithms, such as the Gibbs sampler, have become increasingly popular statistical tools, both in applied and theoretical work. The properties of such algorithms, however, may sometimes not be obvious. Here we give a simple explanation of how and why the Gibbs sampler works. We analytically establish its properties in a simple case and provide insight for more complicated cases. There are also a number of examples.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/W73LAAM5/Casella y George - Explaining the Gibbs Sampler.pdf},
  journal = {The American Statistician},
  number = {3}
}

@article{chib1995,
  title = {Understanding the Metropolis-Hastings Algorithm},
  author = {Chib, Siddhartha and Greenberg, Edward},
  year = {1995},
  month = nov,
  volume = {49},
  pages = {327--335},
  doi = {10.1080/00031305.1995.10476177},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/5CFZZXCL/Greenberg - Understanding the Metropolis-Hastings Algorithm.pdf},
  journal = {American Statistician}
}

@article{cucker2001,
  title = {On the Mathematical Foundations of Learning},
  author = {Cucker, F. and Smale, S.},
  year = {2001},
  month = oct,
  volume = {39},
  pages = {1--50},
  issn = {0273-0979},
  doi = {10.1090/S0273-0979-01-00923-5},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/84MLBZF7/Cucker y Smale - 2001 - On the mathematical foundations of learning.pdf},
  journal = {Bulletin of the American Mathematical Society},
  language = {en},
  number = {01}
}

@book{cucker2007,
  title = {Learning {{Theory}} : {{An Approximation Theory Viewpoint}}},
  author = {Cucker, F. and Zhou, D.},
  year = {2007},
  publisher = {{Cambridge University Press}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/DAUDRJ5V/Cucker y Zhou - Learning Theory  An Approximation Theory Viewpoin.pdf},
  isbn = {978-0-511-27551-7},
  language = {en}
}

@article{damblin2013,
  title = {Numerical Studies of Space Filling Designs: Optimization of {{Latin Hypercube Samples}} and Subprojection Properties},
  shorttitle = {Numerical Studies of Space Filling Designs},
  author = {Damblin, G. and Couplet, M. and Iooss, B.},
  year = {2013},
  month = jul,
  abstract = {Quantitative assessment of the uncertainties tainting the results of computer simulations is nowadays a major topic of interest in both industrial and scientific communities. One of the key issues in such studies is to get information about the output when the numerical simulations are expensive to run. This paper considers the problem of exploring the whole space of variations of the computer model input variables in the context of a large dimensional exploration space. Various properties of space filling designs are justified: interpoint-distance, discrepancy, minimum spanning tree criteria. A specific class of design, the optimized Latin Hypercube Sample, is considered. Several optimization algorithms, coming from the literature, are studied in terms of convergence speed, robustness to subprojection and space filling properties of the resulting design. Some recommendations for building such designs are given. Finally, another contribution of this paper is the deep analysis of the space filling properties of the design 2D-subprojections.},
  archivePrefix = {arXiv},
  eprint = {1307.6835},
  eprinttype = {arxiv},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/C5PFUQEI/Damblin et al. - 2013 - Numerical studies of space filling designs optimi.pdf},
  journal = {arXiv:1307.6835 [math, stat]},
  keywords = {Mathematics - Statistics Theory},
  language = {en},
  primaryClass = {math, stat}
}

@book{debnath1990,
  title = {Introduction to {{Hilbert}} Spaces with Applications},
  author = {Debnath, L. and Mikusinski, P.},
  year = {1990},
  publisher = {{Boston, Mass. : Academic Press, Harcourt Brace Janovich.}},
  address = {{Boston, Mass.}},
  isbn = {0-12-208435-7 978-0-12-208435-5},
  language = {English}
}

@book{dekking2005,
  title = {A Modern Introduction to Probability and Statistics: Understanding Why and How},
  shorttitle = {A Modern Introduction to Probability and Statistics},
  editor = {Dekking, Michel},
  year = {2005},
  publisher = {{Springer}},
  address = {{London}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/GD9Y67KD/Dekking - 2005 - A modern introduction to probability and statistic.pdf},
  isbn = {978-1-85233-896-1},
  keywords = {Mathematical statistics,Probabilities,Textbooks},
  language = {en},
  lccn = {QA273 .M645 2005},
  series = {Springer Texts in Statistics}
}

@book{demmel1997,
  title = {Applied Numerical Linear Algebra},
  author = {Demmel, James W.},
  year = {1997},
  publisher = {{SIAM, Soc. for Industrial and Applied Mathematics}},
  address = {{Philadelphia}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/WQPHSJ29/Demmel.pdf},
  isbn = {0-89871-389-7 978-0-89871-389-3},
  language = {English}
}

@article{dueck1990,
  title = {Threshold Accepting: {{A}} General Purpose Optimization Algorithm Appearing Superior to Simulated Annealing},
  author = {Dueck, Gunter and Scheuer, Tobias},
  year = {1990},
  volume = {90},
  pages = {161--175},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(90)90201-B},
  abstract = {A new general purpose algorithm for the solution of combinatorial optimization problems is presented. The new threshold accepting method is even simpler structured than the wellknown simulated annealing approach. The power of the new algorithm is demonstrated by computational results concerning the traveling salesman problem and the problem of the construction of error-correcting codes. Moreover, deterministic (!) versions of the new heuristic turn out to perform nearly equally well, consuming only a fraction of the computing time of the stochastic versions. As an example, the deterministic threshold accepting method yields very-near-to-optimum tours for the famous 442-cities traveling salesman problem of Gr\"otschel within 1 to 2 s of CPU time.},
  journal = {Journal of Computational Physics},
  number = {1}
}

@book{dulikravich2000,
  title = {Inverse {{Problems}} in {{Engineering Mechanics II}}},
  author = {Dulikravich, G. S. and Tanaka, Mana},
  year = {2000},
  month = dec,
  publisher = {{Elsevier}},
  abstract = {Inverse Problems are found in many areas of engineering mechanics and there are many successful applications e.g. in non-destructive testing and characterization of material properties by ultrasonic or X-ray techniques, thermography, etc. Generally speaking, inverse problems are concerned with the determination of the input and the characteristics of a system, given certain aspects of its output. Mathematically, such problems are ill-posed and have to be overcome through development of new computational schemes, regularization techniques, objective functionals, and experimental procedures. Following the IUTAM Symposium on these topics, held in May 1992 in Tokyo, another in November 1994 in Paris, and also the more recent ISIP'98 in March 1998 in Nagano, it was concluded that it would be fruitful to gather regularly with researchers and engineers for an exchange of the newest research ideas. The most recent Symposium of this series "International Symposium on Inverse Problems in Engineering Mechanics (ISIP2000)" was held in March of 2000 in Nagano, Japan, where recent developments in inverse problems in engineering mechanics and related topics were discussed.The following general areas in inverse problems in engineering mechanics were the subjects of ISIP2000: mathematical and computational aspects of inverse problems, parameter or system identification, shape determination, sensitivity analysis, optimization, material property characterization, ultrasonic non-destructive testing, elastodynamic inverse problems, thermal inverse problems, and other engineering applications. The papers in these proceedings provide a state-of-the-art review of the research on inverse problems in engineering mechanics and it is hoped that some breakthrough in the research can be made and that technology transfer will be stimulated and accelerated due to their publication.},
  isbn = {978-0-08-053515-9},
  keywords = {Technology \& Engineering / Mechanical},
  language = {en}
}

@book{dulikravich2001,
  title = {Inverse {{Problems}} in {{Engineering Mechanics III}}},
  author = {Dulikravich, G. S. and Tanaka, Mana},
  year = {2001},
  month = nov,
  publisher = {{Elsevier}},
  abstract = {Inverse Problems are found in many areas of engineering mechanics and there are many successful applications e.g. in non-destructive testing and characterization of material properties by ultrasonic or X-ray techniques, thermography, etc. Generally speaking, inverse problems are concerned with the determination of the input and the characteristics of a system, given certain aspects of its output. Mathematically, such problems are ill-posed and have to be overcome through development of new computational schemes, regularization techniques, objective functionals, and experimental procedures. This volume contains a selection of peer-reviewed papers presented at the International Symposium on Inverse Problems in Engineering Mechanics (ISIP2001), held in February of 2001 in Nagano, Japan, where recent development in inverse problems in engineering mechanics and related topics were discussed. The following general areas in inverse problems in engineering mechanics were the subjects of the ISIP2001: mathematical and computational aspects of inverse problems, parameter or system identification, shape determination, sensitivity analysis, optimization, material property characterization, ultrasonic non-destructive testing, elastodynamic inverse problems, thermal inverse problems, and other engineering applications. These papers can provide a state-of-the-art review of the research on inverse problems in engineering mechanics.},
  isbn = {978-0-08-053514-2},
  keywords = {Computers / Computer Engineering,Computers / Computer Science,Science / Mechanics / Thermodynamics,Technology \& Engineering / Mechanical},
  language = {en}
}

@book{forrester2008,
  title = {Engineering {{Design}} via {{Surrogate Modelling}}: {{A Practical Guide}}},
  shorttitle = {Engineering {{Design}} via {{Surrogate Modelling}}},
  author = {Forrester, Alexander I. J. and S{\'o}bester, Andr{\'a}s and Keane, Andy J.},
  year = {2008},
  month = jul,
  edition = {First},
  publisher = {{Wiley}},
  doi = {10.1002/9780470770801},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/UP6NU7LZ/Forrester et al. - 2008 - Engineering Design via Surrogate Modelling A Prac.pdf},
  isbn = {978-0-470-06068-1 978-0-470-77080-1},
  language = {en}
}

@article{franz2001,
  title = {Best Possible Strategy for Finding Ground States},
  author = {Franz, Astrid and Hoffmann, Karl and Salamon, Peter},
  year = {2001},
  month = jul,
  volume = {86},
  doi = {10.1103/PhysRevLett.86.5219},
  journal = {Physical Review Letters}
}

@article{gal2014,
  title = {Distributed {{Variational Inference}} in {{Sparse Gaussian Process Regression}} and {{Latent Variable Models}}},
  author = {Gal, Yarin and {van der Wilk}, Mark and Rasmussen, Carl E.},
  year = {2014},
  month = sep,
  abstract = {Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.},
  archivePrefix = {arXiv},
  eprint = {1402.1389},
  eprinttype = {arxiv},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/QDQGA62U/Gal et al. - 2014 - Distributed Variational Inference in Sparse Gaussi.pdf},
  journal = {arXiv:1402.1389 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{geman1992,
  title = {Neural {{Networks}} and the {{Bias}}/{{Variance Dilemma}}},
  author = {Geman, S. and Bienenstock, E. and Doursat, R.},
  year = {1992},
  volume = {4},
  pages = {1--58},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/64E4UMCH/Neural_Networks_and_the_BiasVariance_Dilemma_Geman.pdf},
  journal = {Neural Computation},
  keywords = {learning theory},
  number = {1}
}

@article{golub1979,
  title = {Generalized {{Cross}}-{{Validation}} as a {{Method}} for {{Choosing}} a {{Good Ridge Parameter}}},
  author = {Golub, Gene and Heath, Michael and Wahba, Grace},
  year = {1979},
  month = may,
  volume = {21},
  pages = {215--223},
  doi = {10.1080/00401706.1979.10489751},
  abstract = {Consider the ridge estimate ({$\lambda$}) for {$\beta$} in the model unknown, ({$\lambda$}) = (XX + n{$\lambda$}I) Xy. We study the method of generalized cross-validation (GCV) for choosing a good value for {$\lambda$} from the data. The estimate is the minimizer of V({$\lambda$}) given bywhere A({$\lambda$}) = X(XX + n{$\lambda$}I) X . This estimate is a rotation-invariant version of Allen's PRESS, or ordinary cross-validation. This estimate behaves like a risk improvement estimator, but does not require an estimate of {$\sigma$}, so can be used when n - p is small, or even if p {$\geq$} 2 n in certain cases. The GCV method can also be used in subset selection and singular value truncation methods for regression, and even to choose from among mixtures of these methods.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/RSZZ2XXP/Golub et al. - 1979 - Generalized Cross-Validation as a Method for Choos.pdf},
  journal = {Technometrics}
}

@book{golub2013,
  title = {Matrix Computations},
  author = {Golub, Gene H. and Van Loan, Charles F.},
  year = {2013},
  edition = {Fourth edition},
  publisher = {{The Johns Hopkins University Press}},
  address = {{Baltimore}},
  annotation = {OCLC: ocn824733531},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/FSLE3FKP/Golub y Van Loan - 2013 - Matrix computations.pdf},
  isbn = {978-1-4214-0794-4},
  keywords = {Data processing,Matrices},
  language = {en},
  lccn = {QA188 .G65 2013},
  series = {Johns {{Hopkins}} Studies in the Mathematical Sciences}
}

@article{haario2001,
  title = {An {{Adaptive Metropolis Algorithm}}},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  year = {2001},
  month = apr,
  volume = {7},
  pages = {223},
  issn = {13507265},
  doi = {10.2307/3318737},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/8KNY5M9F/Haario et al. - 2001 - An Adaptive Metropolis Algorithm.pdf},
  journal = {Bernoulli},
  language = {en},
  number = {2}
}

@incollection{henderson2003,
  title = {The {{Theory}} and {{Practice}} of {{Simulated Annealing}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Henderson, Darrall and Jacobson, Sheldon H. and Johnson, Alan W.},
  editor = {Glover, Fred and Kochenberger, Gary A.},
  year = {2003},
  volume = {57},
  pages = {287--319},
  publisher = {{Kluwer Academic Publishers}},
  address = {{Boston}},
  doi = {10.1007/0-306-48056-5_10},
  abstract = {Simulated annealing is a popular local search meta-heuristic used to address discrete and, to a lesser extent, continuous optimization problems. The key feature of simulated annealing is that it provides a means to escape local optima by allowing hill-climbing moves (i.e., moves which worsen the objective function value) in hopes of finding a global optimum. A brief history of simulated annealing is presented, including a review of its application to discrete and continuous optimization problems. Convergence theory for simulated annealing is reviewed, as well as recent advances in the analysis of finite time performance. Other local search algorithms are discussed in terms of their relationship to simulated annealing. The chapter also presents practical guidelines for the implementation of simulated annealing in terms of cooling schedules, neighborhood functions, and appropriate applications.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/YITWMN4V/Henderson et al. - 2003 - The Theory and Practice of Simulated Annealing.pdf},
  isbn = {978-1-4020-7263-5},
  language = {en}
}

@article{hensman2013,
  title = {Gaussian Processes for Big Data},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil},
  year = {2013},
  month = sep,
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/3TSCK2A8/Hensman et al. - Gaussian Processes for Big Data.pdf},
  journal = {Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013}
}

@book{hochstadt1989,
  title = {Integral Equations},
  author = {Hochstadt, Harry.},
  year = {1989},
  publisher = {{Wiley}},
  address = {{New York}},
  language = {English}
}

@article{hoerl1970,
  title = {Ridge {{Regression}}: {{Biased Estimation}} for {{Nonorthogonal Problems}}},
  shorttitle = {Ridge {{Regression}}},
  author = {Hoerl, Arthur E. and Kennard, Robert W.},
  year = {1970},
  month = feb,
  volume = {12},
  pages = {55--67},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1970.10488634},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/ZSH4AWMA/Hoerl y Kennard - 1970 - Ridge Regression Biased Estimation for Nonorthogo.pdf},
  journal = {Technometrics},
  language = {en},
  number = {1}
}

@article{hofmann2008,
  title = {Kernel Methods in Machine Learning},
  author = {Hofmann, Thomas and Sch{\"o}lkopf, Bernhard and Smola, Alexander J.},
  year = {2008},
  month = jun,
  volume = {36},
  pages = {1171--1220},
  issn = {0090-5364},
  doi = {10.1214/009053607000000677},
  abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
  archivePrefix = {arXiv},
  eprint = {math/0701907},
  eprinttype = {arxiv},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/T369ZKTH/Hofmann et al. - 2008 - Kernel methods in machine learning.pdf},
  journal = {The Annals of Statistics},
  keywords = {30C40 (Primary) 68T05 (Secondary),Mathematics - Probability,Mathematics - Statistics Theory},
  language = {en},
  number = {3}
}

@article{ingber1989,
  title = {Very Fast Simulated Re-Annealing},
  author = {Ingber, L.},
  year = {1989},
  volume = {12},
  pages = {967--973},
  issn = {08957177},
  doi = {10.1016/0895-7177(89)90202-1},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/6P5WMGAQ/Ingber - 1989 - Very fast simulated re-annealing.pdf},
  journal = {Mathematical and Computer Modelling},
  language = {en},
  number = {8}
}

@article{ingber2000,
  title = {Adaptive Simulated Annealing ({{ASA}}): {{Lessons}} Learned},
  shorttitle = {Adaptive Simulated Annealing ({{ASA}})},
  author = {Ingber, Lester},
  year = {2000},
  month = jan,
  abstract = {Adaptive simulated annealing (ASA) is a global optimization algorithm based on an associated proof that the parameter space can be sampled much more efficiently than by using other previous simulated annealing algorithms. The author's ASA code has been publicly available for over two years. During this time the author has volunteered to help people via e-mail, and the feedback obtained has been used to further develop the code. Some lessons learned, in particular some which are relevant to other simulated annealing algorithms, are described.},
  archivePrefix = {arXiv},
  eprint = {cs.MS/0001018},
  eprinttype = {arxiv},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/98FS7PK7/Ingber - 2000 - Adaptive simulated annealing (ASA) Lessons learne.pdf},
  journal = {arXiv:cs.MS/0001018},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Computer Science - Mathematical Software,G.1.6},
  language = {en}
}

@article{justel1997,
  title = {A Multivariate {{Kolmogorov}}-{{Smirnov}} Test of Goodness of Fit},
  author = {Justel, Ana and Pe{\~n}a, Daniel and Zamar, Rub{\'e}n},
  year = {1997},
  month = oct,
  volume = {35},
  pages = {251--259},
  issn = {01677152},
  doi = {10.1016/S0167-7152(97)00020-5},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/3QCD8YCE/Justel et al. - 1997 - A multivariate Kolmogorov-Smirnov test of goodness.pdf},
  journal = {Statistics \& Probability Letters},
  language = {en},
  number = {3}
}

@article{kapteyn2019,
  title = {Distributionally Robust Optimization for Engineering Design under Uncertainty},
  author = {Kapteyn, Michael G. and Willcox, Karen E. and Philpott, Andy B.},
  year = {2019},
  month = nov,
  volume = {120},
  pages = {835--859},
  issn = {0029-5981, 1097-0207},
  doi = {10.1002/nme.6160},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/E5TMKSHL/Kapteyn et al. - 2019 - Distributionally robust optimization for engineeri.pdf},
  journal = {International Journal for Numerical Methods in Engineering},
  language = {en},
  number = {7}
}

@article{kent1982,
  title = {The {{Fisher}}-{{Bingham Distribution}} on the {{Sphere}}},
  author = {Kent, John T.},
  year = {1982},
  month = sep,
  volume = {44},
  pages = {71--80},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1982.tb01189.x},
  abstract = {The Fisher distribution is the analogue on the sphere of the isotropic bivariate normal distribution in the plane. The purpose of this paper is to propose and analyse a spherical analogue of the general bivariate normal distribution. Estimation, hypothesis testing and confidence regions are also discussed.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/ZZZ4IDTH/Kent - 1982 - The Fisher-Bingham Distribution on the Sphere.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  language = {en},
  number = {1}
}

@article{kimeldorf1971,
  title = {Some Results on {{Tchebycheffian}} Spline Functions},
  author = {Kimeldorf, George and Wahba, Grace},
  year = {1971},
  month = jan,
  volume = {33},
  pages = {82--95},
  issn = {0022-247X},
  doi = {10.1016/0022-247X(71)90184-3},
  abstract = {This report derives explicit solutions to problems involving Tchebycheffian spline functions. We use a reproducing kernel Hilbert space which depends on the smoothness criterion, but not on the form of the data, to solve explicitly Hermite-Birkhoff interpolation and smoothing problems. Sard's best approximation to linear functionals and smoothing with respect to linear inequality constraints are also discussed. Some of the results are used to show that spline interpolation and smoothing is equivalent to prediction and filtering on realizations of certain stochastic processes.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/2SNMDRPA/Kimeldorf y Wahba - 1971 - Some results on Tchebycheffian spline functions.pdf;/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/EYYM86NA/0022247X71901843.html},
  journal = {Journal of Mathematical Analysis and Applications},
  language = {en},
  number = {1}
}

@book{knuth1969,
  title = {The Art of Computer Programming. {{Volume}} 2.},
  author = {Knuth, Donald E.},
  year = {1969},
  publisher = {{Addison-Wesley}},
  address = {{Reading, Mass}},
  language = {English}
}

@book{konig1986,
  title = {Eigenvalue Distribution of Compact Operators},
  author = {K{\"o}nig, H.},
  year = {1986},
  publisher = {{Birkh\"auser Verlag}},
  address = {{Basel; Boston}},
  isbn = {3-7643-1755-8 978-3-7643-1755-3},
  language = {English}
}

@book{krauth2006,
  title = {Statistical Mechanics: Algorithms and Computations},
  shorttitle = {Statistical Mechanics},
  author = {Krauth, Werner},
  year = {2006},
  publisher = {{Oxford Univ. Press}},
  address = {{Oxford}},
  annotation = {OCLC: 254061236},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/7EEDXRG4/Krauth - 2006 - Statistical mechanics algorithms and computations.pdf},
  isbn = {978-0-19-851535-7 978-0-19-851536-4},
  language = {en},
  number = {13},
  series = {Oxford Master Series in Physics {{Statistical}}, Computational, and Theoretical Physics}
}

@inproceedings{lazaro-gredilla2011,
  title = {Variational Heteroscedastic Gaussian Process Regression.},
  author = {{L{\'a}zaro-Gredilla}, Miguel and Titsias, Michalis},
  year = {2011},
  month = jan,
  pages = {841--848},
  series = {Proceedings of the 28th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2011}
}

@article{li2017,
  title = {A Novel Extension Algorithm for Optimized {{Latin}} Hypercube Sampling},
  author = {Li, W. and Lu, L. and Xie, X. and Yang, M.},
  year = {2017},
  month = sep,
  volume = {87},
  pages = {2549--2559},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949655.2017.1340475},
  abstract = {Latin hypercube sampling (LHS), as an efficient sampling method, has been widely used in computer experiments. But it is difficult to choice the sample size while applying LHS, especially for expensive simulations. The effective way is to add sample points sequentially. Nevertheless, the oversampling problem may be countered while extending the sample size with the existing extension algorithms of LHS. To alleviate this problem and obtain extension sample with good space-filling properties, a novel extension algorithm of optimized LHS (OLHS) is proposed. According to the extending rule, a new LHS is constructed by adding sample points of size n each time firstly. Then each additional sample points are optimized by the enhanced stochastic evolutionary algorithm based on the CL2 space-filling criterion. The extension algorithm is illustrated by two test functions and appears to perform well in both efficiency and convergence compared with the traditional extension algorithm.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/AQR4ZDKZ/Li et al. - 2017 - A novel extension algorithm for optimized Latin hy.pdf},
  journal = {Journal of Statistical Computation and Simulation},
  language = {en},
  number = {13}
}

@article{liu2020,
  title = {When {{Gaussian Process Meets Big Data}}: {{A Review}} of {{Scalable GPs}}},
  shorttitle = {When {{Gaussian Process Meets Big Data}}},
  author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  year = {2020},
  pages = {1--19},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2019.2957109},
  abstract = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/PYBTTC44/Liu et al. - 2020 - When Gaussian Process Meets Big Data A Review of .pdf},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  language = {en}
}

@article{mackay1998,
  title = {Introduction to {{Gaussian}} Processes},
  author = {MacKay, David JC},
  year = {1998},
  volume = {168},
  pages = {133--166},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/8JVV4AXR/introductionGaussianProcessesMackay.pdf;/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/BKCF732N/MacKay - 1998 - Introduction to Gaussian processes.pdf},
  journal = {NATO ASI series F computer and systems sciences}
}

@article{mardia1975,
  title = {Statistics of {{Directional Data}}},
  author = {Mardia, K. V.},
  year = {1975},
  volume = {37},
  pages = {349--371},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1975.tb01550.x},
  abstract = {Directional data analysis is emerging as an important area of statistics. Within the past two decades, various new techniques have appeared, mostly to meet the needs of scientific workers dealing with directional data. The paper first introduces the two basic models for the multi-dimensional case known as the von Mises\textendash Fisher distribution and the Bingham distribution. Their sampling distribution theory depends heavily on the isotropic case and some developments are discussed. An optimum property of an important test for the von Mises\textendash Fisher case is established. A non-parametric test is proposed for the hypothesis of independence for observations on a torus. In addition to some numerical examples on the preceding topics, five case studies are given which illuminate the power of this new methodology. The case studies are concerned with cancer research, origins of comets, arrival times of patients, navigational problems and biological rhythms. Some unsolved problems are also indicated.},
  copyright = {\textcopyright{} 1975 The Authors},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/DJGWNKX4/Mardia - 2020 - Statistics of Directional Data.pdf;/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/JTG3ZMD8/j.2517-6161.1975.tb01550.html},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  keywords = {arrival times,bingham distribution,biological rhythms,bird navigation,cancer cells,characterization,correlation on torus,directional data,independence,origin of comets,random walk,uniform scores,von misesâ€“fisher distribution},
  language = {en},
  number = {3}
}

@article{marsaglia1964,
  title = {A Convenient Method for Generating Normal Variables},
  author = {Marsaglia, G. and Bray, T.},
  year = {1964},
  month = jul,
  volume = {6},
  pages = {260--264},
  doi = {10.1137/1006063},
  journal = {Siam Review - SIAM REV}
}

@article{mercer1909,
  title = {Functions of Positive and Negative Type, and Their Connection with the Theory of Integral Equations},
  author = {Mercer, J.},
  year = {1909},
  volume = {209},
  pages = {415--446},
  journal = {Philosophical Transactions of the Royal Society, London},
  keywords = {MachineLearning intelligenza-artificiale imported svm}
}

@article{metropolis1953,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  year = {1953},
  month = jun,
  volume = {21},
  pages = {1087--1092},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1699114},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/N28XVWM9/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf;/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/J9RDIPEF/1.html},
  journal = {The Journal of Chemical Physics},
  number = {6}
}

@article{micchelli2005,
  title = {On {{Learning Vector}}-{{Valued Functions}}},
  author = {Micchelli, Charles A. and Pontil, Massimiliano},
  year = {2005},
  month = jan,
  volume = {17},
  pages = {177--204},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/0899766052530802},
  abstract = {In this paper, we provide a study of learning in a Hilbert space of vector-valued functions. We motivate the need for extending learning theory of scalar-valued functions by practical considerations and establish some basic results for learning vector-valued functions which should prove useful in applications. Specifically, we allow an output space Y to be a Hilbert space and we consider a reproducing kernel Hilbert space of functions whose values lie in Y. In this setting, we derive the form of the minimal norm interpolant to a finite set of data and apply it to study some regularization functionals which are important in learning theory. We consider specific examples of such functionals corresponding to multiple-output regularization networks and support vector machines, both for regression and classification. Finally, we provide classes of operator-valued kernels of the dot product and translation invariant type.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/N2J93TBE/Micchelli y Pontil - 2005 - On Learning Vector-Valued Functions.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {1}
}

@book{mises1964,
  title = {Mathematical Theory of Probability and Statistics},
  author = {von. Mises, Richard and Geiringer, Hilda.},
  year = {1964},
  publisher = {{Academic Press}},
  address = {{New York}},
  language = {English}
}

@book{mohri2018,
  title = {Foundations of Machine Learning},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/GSJN4I28/Mohri et al. - 2018 - Foundations of machine learning.pdf},
  isbn = {978-0-262-03940-6},
  keywords = {Computer algorithms,Machine learning},
  language = {en},
  lccn = {Q325.5 .M64 2018},
  series = {Adaptive Computation and Machine Learning}
}

@article{muandet2017,
  title = {Kernel {{Mean Embedding}} of {{Distributions}}: {{A Review}} and {{Beyond}}},
  shorttitle = {Kernel {{Mean Embedding}} of {{Distributions}}},
  author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  volume = {10},
  pages = {1--141},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000060},
  abstract = {A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/5PQJ3BB5/Muandet et al. - 2017 - Kernel Mean Embedding of Distributions A Review a.pdf},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  number = {1-2}
}

@article{nannapaneni2020,
  title = {Probability-Space Surrogate Modeling for Fast Multidisciplinary Optimization under Uncertainty},
  author = {Nannapaneni, Saideep and Mahadevan, Sankaran},
  year = {2020},
  month = jun,
  volume = {198},
  pages = {106896},
  issn = {09518320},
  doi = {10.1016/j.ress.2020.106896},
  abstract = {This paper proposes a probability-space surrogate modeling approach for computationally efficient multidisciplinary design optimization under uncertainty. This paper uses a probability-space surrogate as opposed to an algebraic surrogate so that the probability distributions of the required outputs at a given design input can naturally be obtained without repeated Monte Carlo runs of an algebraic surrogate at different realizations of the uncertain variables. We consider three probability-space surrogates with analytical solutions for prediction and inference - Multivariate Gaussian, Gaussian Copula, and Gaussian Mixture Model, and investigate their applicability to perform multidisciplinary design optimization under uncertainty. All the input design and random variables, coupling variables, objective and constraint functions are incorporated within the probability-space surrogate, which helps analytically obtain the distributions of coupling variables, objective and constraint functions at desired design inputs while enforcing multidisciplinary compatibility. The training points for the probability-space surrogates are obtained by performing one-pass analysis through the disciplinary models at different realizations of the input variables. The proposed methodology is demonstrated for reliability-based design optimization (RBDO) and reliability-based robust design optimization (RBRDO) in an aircraft design example. The performance of the probability-space surrogates is compared against a Kriging algebraic surrogate and fully coupled Monte Carlo analysis.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/YNC6KUA3/Nannapaneni y Mahadevan - 2020 - Probability-space surrogate modeling for fast mult.pdf},
  journal = {Reliability Engineering \& System Safety},
  language = {en}
}

@book{neto2012,
  title = {An {{Introduction}} to {{Inverse Problems}} with {{Applications}}},
  author = {Neto, Francisco Duarte Moura and Neto, Ant{\^o}nio Jos{\'e} da Silva},
  year = {2012},
  month = sep,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Computational engineering/science uses a blend of applications, mathematical models and computations. Mathematical models require accurate approximations of their parameters, which are often viewed as solutions to inverse problems. Thus, the study of inverse problems is an integral part of computational engineering/science. This book presents several aspects of inverse problems along with needed prerequisite topics in numerical analysis and matrix algebra. If the reader has previously studied these prerequisites, then one can rapidly move to the inverse problems in chapters 4-8 on image restoration, thermal radiation, thermal characterization and heat transfer.``This text does provide a comprehensive introduction to inverse problems and fills a void in the literature''.Robert E White, Professor of Mathematics, North Carolina State University},
  isbn = {978-3-642-32556-4},
  keywords = {Computers / Computer Science,Mathematics / Applied,Science / Mechanics / Thermodynamics,Technology \& Engineering / General},
  language = {en}
}

@article{osullivan1986,
  title = {Automatic {{Smoothing}} of {{Regression Functions}} in {{Generalized Linear Models}}},
  author = {O'sullivan, Finbarr and Yandell, Brian S. and William, J. Raynor},
  year = {1986},
  month = mar,
  volume = {81},
  pages = {96--103},
  issn = {0162-1459},
  doi = {10.1080/01621459.1986.10478243},
  abstract = {We consider the penalized likelihood method for estimating nonparametric regression functions in generalized linear models (Nelder and Wedderburn 1972) and present a generalized cross-validation procedure for empirically assessing an appropriate amount of smoothing in these estimates. Asymptotic arguments and numerical simulations are used to show that the generalized cross-validatory procedure preforms well from the point of view of a weighted mean squared error criterion. The methodology adds to the battery of graphical tools for model building and checking within the generalized linear model framework. Included are two examples motivated by medical and horticultural applications.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/9TYLKYQ8/01621459.1986.html},
  journal = {Journal of the American Statistical Association},
  keywords = {Cross-validation,IRLS,Penalized likelihood,Smoothing splines},
  number = {393}
}

@book{press1992,
  title = {Numerical Recipes in {{C}} : The Art of Scientific Computing},
  author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
  year = {1992},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge [etc.]}},
  isbn = {0-521-43108-5 978-0-521-43108-8},
  language = {English}
}

@book{rao2009,
  title = {Engineering Optimization: Theory and Practice},
  shorttitle = {Engineering Optimization},
  author = {Rao, Singiresu S.},
  year = {2009},
  edition = {4th ed},
  publisher = {{John Wiley \& Sons}},
  address = {{Hoboken, N.J}},
  annotation = {OCLC: ocn320352991},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/NWQFH4ZE/Rao - 2009 - Engineering optimization theory and practice.pdf},
  isbn = {978-0-470-18352-6},
  keywords = {Engineering,Mathematical models,Mathematical optimization},
  language = {en},
  lccn = {TA342 .R36 2009}
}

@book{rasmussen2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  annotation = {OCLC: ocm61285753},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/638QAH2H/Rasmussen y Williams - 2006 - Gaussian processes for machine learning.pdf},
  isbn = {978-0-262-18253-9},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  language = {en},
  lccn = {QA274.4 .R37 2006},
  series = {Adaptive Computation and Machine Learning}
}

@article{rescorla2015,
  title = {Some Epistemological Ramifications of the {{Borel}}\textendash{{Kolmogorov}} Paradox},
  author = {Rescorla, Michael},
  year = {2015},
  month = mar,
  volume = {192},
  pages = {735--767},
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-014-0586-z},
  abstract = {This paper discusses conditional probability P(A|B), or the probability of A given B. When P(B) {$>$} 0, the ratio formula determines P(A|B). When P(B) = 0, the ratio formula breaks down. The Borel\textendash Kolmogorov paradox suggests that conditional probabilities in such cases are indeterminate or ill-posed. To analyze the paradox, I explore the relation between probability and intensionality. I argue that the paradox is a Frege case, similar to those that arise in many probabilistic and non-probabilistic contexts. The paradox vividly illustrates how an agent's way of representing an entity can rationally influence her credal assignments. I deploy my analysis to defend Kolmogorov's relativistic treatment of conditional probability.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/JCBD2VIM/Rescorla - 2015 - Some epistemological ramifications of the Borelâ€“Ko.pdf},
  journal = {Synthese},
  language = {en},
  number = {3}
}

@book{robert2004,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P. and Casella, George},
  year = {2004},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-4145-2},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/ACCG5Y4B/Robert y Casella - 2004 - Monte Carlo Statistical Methods.pdf},
  isbn = {978-1-4419-1939-7 978-1-4757-4145-2},
  language = {en},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@book{santner2003,
  title = {The {{Design}} and {{Analysis}} of {{Computer Experiments}}},
  author = {Santner, T. J. and Williams, B. J. and Notz, W. I.},
  year = {2003},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-3799-8},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/GNKQQDJT/Santner et al. - The Design and Analysis of Computer Experiments.pdf},
  isbn = {978-1-4419-2992-1 978-1-4757-3799-8},
  language = {en},
  series = {Springer {{Series}} in {{Statistics}}}
}

@article{schoenberg1964,
  title = {Spline Functions and the Problem of Graduation},
  author = {Schoenberg, I. J.},
  year = {1964},
  month = oct,
  volume = {52},
  pages = {947--950},
  issn = {0027-8424},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/7U8GY3BH/Schoenberg - 1964 - SPLINE FUNCTIONS AND THE PROBLEM OF GRADUATION.pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  number = {4},
  pmcid = {PMC300377},
  pmid = {16591233}
}

@article{schoenberg1988,
  title = {Metric {{Spaces}} and {{Completely Monotone Functions}}},
  author = {Schoenberg, I. J.},
  year = {1988},
  pages = {115--145},
  language = {English}
}

@incollection{scholkopf2001,
  title = {A {{Generalized Representer Theorem}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Helmbold, David and Williamson, Bob},
  year = {2001},
  volume = {2111},
  pages = {416--426},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44581-1_27},
  abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/VE8HITX6/SchÃ¶lkopf et al. - 2001 - A Generalized Representer Theorem.pdf},
  isbn = {978-3-540-42343-0 978-3-540-44581-4},
  language = {en}
}

@book{scholkopf2002,
  title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  shorttitle = {Learning with Kernels},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander J.},
  year = {2002},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/LVN2RITS/SchÃ¶lkopf y Smola - 2002 - Learning with kernels support vector machines, re.pdf},
  isbn = {978-0-262-19475-4},
  keywords = {Kernel functions,Support vector machines},
  language = {en},
  lccn = {Q325.5 .S32 2002},
  series = {Adaptive Computation and Machine Learning}
}

@incollection{scholkopf2003,
  title = {A {{Short Introduction}} to {{Learning}} with {{Kernels}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander J.},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Mendelson, Shahar and Smola, Alexander J.},
  year = {2003},
  volume = {2600},
  pages = {41--64},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36434-X_2},
  abstract = {We briefly describe the main ideas of statistical learning theory, support vector machines, and kernel feature spaces. This includes a derivation of the support vector optimization problem for classification and regression, the {$\nu$}-trick, various kernels and an overview over applications of kernel methods.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/CL6RP38F/SchÃ¶lkopf y Smola - 2003 - A Short Introduction to Learning with Kernels.pdf},
  isbn = {978-3-540-00529-2 978-3-540-36434-4},
  language = {en}
}

@article{siarry1997,
  title = {Enhanced Simulated Annealing for Globally Minimizing Functions of Many-Continuous Variables},
  author = {Siarry, Patrick and Berthiau, G{\'e}rard and Durdin, Fran{\c c}ois and Haussy, Jacques},
  year = {1997},
  month = jun,
  volume = {23},
  pages = {209--228},
  issn = {0098-3500, 1557-7295},
  doi = {10.1145/264029.264043},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/I4TS643C/Siarry et al. - 1997 - Enhanced simulated annealing for globally minimizi.pdf},
  journal = {ACM Transactions on Mathematical Software (TOMS)},
  language = {en},
  number = {2}
}

@article{simpson2001,
  title = {Kriging {{Models}} for {{Global Approximation}} in {{Simulation}}-{{Based Multidisciplinary Design Optimization}}},
  author = {Simpson, Timothy W. and Mauery, Timothy M. and Korte, John J. and Mistree, Farrokh},
  year = {2001},
  volume = {39},
  pages = {2233--2241},
  issn = {0001-1452},
  doi = {10.2514/2.1234},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/AXVFDBYZ/Simpson et al. - 2001 - Kriging Models for Global Approximation in Simulat.pdf;/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/JI4ENENP/2.html},
  journal = {AIAA Journal},
  number = {12}
}

@article{smale2003,
  title = {Estimating the Approximation Error in {{Learning Theory}}},
  author = {Smale, Steve and Zhou, Ding-Xuan},
  year = {2003},
  month = jan,
  volume = {01},
  pages = {17--41},
  issn = {0219-5305, 1793-6861},
  doi = {10.1142/S0219530503000089},
  abstract = {Let B be a Banach space and ({$\mathscr{H}$},{$\Vert\cdot\Vert$}               {$\mathscr{H}$}               ) be a dense, imbedded subspace. For a {$\in$} B, its distance to the ball of {$\mathscr{H}$} with radius R (denoted as I(a, R)) tends to zero when R tends to infinity. We are interested in the rate of this convergence. This approximation problem arose from the study of learning theory, where B is the L               2               space and {$\mathscr{H}$} is a reproducing kernel Hilbert space.                                         The class of elements having I(a, R) = O(R               -r               ) with r {$>$} 0 is an interpolation space of the couple (B, {$\mathscr{H}$}). The rate of convergence can often be realized by linear operators. In particular, this is the case when {$\mathscr{H}$} is the range of a compact, symmetric, and strictly positive definite linear operator on a separable Hilbert space B. For the kernel approximation studied in Learning Theory, the rate depends on the regularity of the kernel function. This yields error estimates for the approximation by reproducing kernel Hilbert spaces. When the kernel is smooth, the convergence is slow and a logarithmic convergence rate is presented for analytic kernels in this paper. The purpose of our results is to provide some theoretical estimates, including the constants, for the approximation error required for the learning theory.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/IALGMQN7/Smale y Zhou - 2003 - ESTIMATING THE APPROXIMATION ERROR IN LEARNING THE.pdf},
  journal = {Analysis and Applications},
  language = {en},
  number = {01}
}

@article{smith1993,
  title = {Bayesian {{Computation Via}} the {{Gibbs Sampler}} and {{Related Markov Chain Monte Carlo Methods}}},
  author = {Smith, A. F. M. and Roberts, G. O.},
  year = {1993},
  volume = {55},
  pages = {3--23},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {The use of the Gibbs sampler for Bayesian computation is reviewed and illustrated in the context of some canonical examples. Other Markov chain Monte Carlo simulation methods are also briefly described, and comments are made on the advantages of sample-based approaches for Bayesian inference summaries.},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number = {1}
}

@phdthesis{song2008,
  title = {Learning via {{Hilbert Space Embedding}} of {{Distributions}}},
  author = {Song, Le},
  year = {2008},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/E6SGQHM6/LearningViaHilbertSpaceEmbeddingOfDistributions_Song_PhD_thesis.pdf},
  school = {University of Sydney}
}

@book{spall2003,
  title = {Introduction to Stochastic Search and Optimization : Estimation, Simulation, and Control},
  author = {Spall, James C.},
  year = {2003},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  isbn = {0-471-33052-3 978-0-471-33052-3},
  language = {English}
}

@book{stirzaker2007,
  title = {Elementary Probability},
  author = {Stirzaker, David.},
  year = {2007},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/ML2TJLZ4/Stirzaker - Elementary Probability.pdf},
  isbn = {0-521-53428-3 978-0-521-53428-4},
  language = {English}
}

@book{tanaka1998,
  title = {Inverse {{Problems}} in {{Engineering Mechanics}}},
  author = {Tanaka, Masataka and Dulikravich, G. S.},
  year = {1998},
  month = nov,
  publisher = {{Elsevier}},
  abstract = {Inverse problems can be found in many topics of engineering mechanics. There are many successful applications in the fields of inverse problems (non-destructive testing and characterization of material properties by ultrasonic or X-ray techniques, thermography, etc.). Generally speaking, the inverse problems are concerned with the determination of the input and the characteristics of a mechanical system from some of the output from the system. Mathematically, such problems are ill-posed and have to be overcome through development of new computational schemes, regularization techniques, objective functionals, and experimental procedures.Seventy-two papers were presented at the International Symposium on Inverse Problems in Mechanics (ISIP '98) held in March of 1998 in Nagano, where recent developments in the inverse problems in engineering mechanics and related topics were discussed. The main themes were: mathematical and computational aspects of the inverse problems, parameter or system identification, shape determination, sensitivity analysis, optimization, material property characterization, ultrasonic non-destructive testing, elastodynamic inverse problems, thermal inverse problems, and other engineering applications.},
  isbn = {978-0-08-053516-6},
  keywords = {Technology \& Engineering / Mechanical},
  language = {en}
}

@book{tanaka2003,
  title = {Inverse {{Problems}} in {{Engineering Mechanics IV}}},
  shorttitle = {Inverse {{Problems}} in {{Engineering Mechanics IV}}},
  author = {Tanaka, Mana},
  year = {2003},
  month = nov,
  publisher = {{Elsevier}},
  abstract = {This latest collection of proceedings provides a state of the art review of research on inverse problems in engineering mechanics. Inverse problems can be found in many areas of engineering mechanics, and have many successful applications. They are concerned with estimating the unknown input and/or the characteristics of a system given certain aspects of its output. The mathematical challenges of such problems have to be overcome through the development of new computational schemes, regularization techniques, objective functionals, and experimental procedures. The papers within this represent an excellent reference for all in the field.Providing a state of the art review of research on inverse problems in engineering mechanicsContains the latest research ideas and related techniquesA recognized standard reference in the field of inverse problemsPapers from Asia, Europe and America are all well represented},
  isbn = {978-0-08-053517-3},
  keywords = {Mathematics / Applied,Science / Mechanics / Fluids,Science / Mechanics / Solids,Technology \& Engineering / Engineering (General)},
  language = {en}
}

@book{titterington1985,
  title = {Statistical Analysis of Finite Mixture Distributions},
  author = {Titterington, D. M. and Smith, A. F. M. and Makov, U. E.},
  year = {1985},
  publisher = {{J. Wiley}},
  address = {{Chichester}},
  isbn = {0-471-90763-4 978-0-471-90763-3},
  language = {English}
}

@book{trefethen1997,
  title = {Numerical Linear Algebra},
  author = {Trefethen, Lloyd N. and Bau, David.},
  year = {1997},
  publisher = {{SIAM}},
  address = {{Philadelphia, Pa.}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/VYL2KJNE/Trefethen_Bau.pdf},
  isbn = {0-89871-361-7 978-0-89871-361-9},
  language = {English}
}

@book{vapnik1998,
  title = {Statistical Learning Theory},
  author = {Vapnik, Vladimir Naumovich},
  year = {1998},
  publisher = {{Wiley}},
  address = {{New York}},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/ZMYNCMPT/Vapnik - 1998 - Statistical learning theory.pdf},
  isbn = {978-0-471-03003-4},
  keywords = {Computational learning theory},
  language = {en},
  lccn = {Q325.7 .V38 1998},
  series = {Adaptive and Learning Systems for Signal Processing, Communications, and Control}
}

@article{vonluxburg2008,
  title = {Statistical {{Learning Theory}}: {{Models}}, {{Concepts}}, and {{Results}}},
  shorttitle = {Statistical {{Learning Theory}}},
  author = {{von Luxburg}, Ulrike and Schoelkopf, Bernhard},
  year = {2008},
  month = oct,
  abstract = {Statistical learning theory provides the theoretical basis for many of today's machine learning algorithms. In this article we attempt to give a gentle, non-technical overview over the key ideas and insights of statistical learning theory. We target at a broad audience, not necessarily machine learning researchers. This paper can serve as a starting point for people who want to get an overview on the field before diving into technical details.},
  archivePrefix = {arXiv},
  eprint = {0810.4752},
  eprinttype = {arxiv},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/EIZ85BGT/von Luxburg y Schoelkopf - 2008 - Statistical Learning Theory Models, Concepts, and.pdf},
  journal = {arXiv:0810.4752 [math, stat]},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  language = {en},
  primaryClass = {math, stat}
}

@inproceedings{wang2005,
  title = {Gaussian {{Process Meta}}-{{Models}} for {{Efficient Probabilistic Design}} in {{Complex Engineering Design Spaces}}},
  booktitle = {Volume 2: 31st {{Design Automation Conference}}, {{Parts A}} and {{B}}},
  author = {Wang, Liping and Beeson, Don and Akkaram, Srikanth and Wiggs, Gene},
  year = {2005},
  month = jan,
  pages = {785--798},
  publisher = {{ASMEDC}},
  address = {{Long Beach, California, USA}},
  doi = {10.1115/DETC2005-85406},
  abstract = {Probabilistic design in complex design spaces is often a computationally expensive and difficult task because of the highly nonlinear and noisy nature of those spaces. Approximate probabilistic methods, such as, First-Order Second-Moments (FOSM) and Point Estimate Method (PEM) have been developed to alleviate the high computational cost issue. However, both methods have difficulty with non-monotonic spaces and FOSM may have convergence problems if noise on the space makes it difficult to calculate accurate numerical partial derivatives. Use of design and Analysis of Computer Experiments (DACE) methods to build polynomial meta-models is a common approach which both smoothes the design space and significantly improves the computational efficiency. However, this type of model is inherently limited by the properties of the polynomial function and its transformations. Therefore, polynomial meta-models may not accurately represent the portion of the design space that is of interest to the engineer. The objective of this paper is to utilize Gaussian Process (GP) techniques to build an alternative meta-model that retains the properties of smoothness and fast execution but has a much higher level of accuracy. If available, this high quality GP model can then be used for fast probabilistic analysis based on a function that much more closely represents the original design space. Achieving the GP goal of a highly accurate meta-model requires a level of mathematics that is much more complex than the mathematics required for regular linear and quadratic response surfaces. Many difficult mathematical issues encountered in the implementation of the Gaussian Process meta-model are addressed in this paper. Several selected examples demonstrate the accuracy of the GP models and efficiency improvements related to probabilistic design.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/4N3UGIKV/Wang et al. - 2005 - Gaussian Process Meta-Models for Efficient Probabi.pdf},
  isbn = {978-0-7918-4739-8},
  language = {en}
}

@book{woodbury1950,
  title = {Inverting Modified Matrices},
  author = {Woodbury, Max A.},
  editor = {{Princeton University}},
  year = {1950},
  publisher = {{Princeton, NJ: Department of Statistics, Princeton University}},
  keywords = {Matrices,Statistics},
  language = {English},
  lccn = {PAM I},
  series = {{{SRG Memorandum}} Report ; 42}
}

@article{xiong2009,
  title = {Optimizing {{Latin}} Hypercube Design for Sequential Sampling of Computer Experiments},
  author = {Xiong, F. and Xiong, Y. and Chen, W. and Yang, S.},
  year = {2009},
  month = aug,
  volume = {41},
  pages = {793--810},
  issn = {0305-215X, 1029-0273},
  doi = {10.1080/03052150902852999},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/HVZQMI2Y/Xiong et al. - 2009 - Optimizing Latin hypercube design for sequential s.pdf},
  journal = {Engineering Optimization},
  language = {en},
  number = {8}
}

@book{yang2010,
  title = {Engineering {{Optimization}}: {{An Introduction}} with {{Metaheuristic Applications}}},
  shorttitle = {Engineering {{Optimization}}},
  author = {Yang, Xin-She},
  year = {2010},
  month = jul,
  publisher = {{John Wiley \& Sons}},
  abstract = {An accessible introduction to metaheuristics and optimization, featuring powerful and modern algorithms for application across engineering and the sciences From engineering and computer science to economics and management science, optimization is a core component for problem solving. Highlighting the latest developments that have evolved in recent years, Engineering Optimization: An Introduction with Metaheuristic Applications outlines popular metaheuristic algorithms and equips readers with the skills needed to apply these techniques to their own optimization problems. With insightful examples from various fields of study, the author highlights key concepts and techniques for the successful application of commonly-used metaheuristc algorithms, including simulated annealing, particle swarm optimization, harmony search, and genetic algorithms. The author introduces all major metaheuristic algorithms and their applications in optimization through a presentation that is organized into three succinct parts:  Foundations of Optimization and Algorithms provides a brief introduction to the underlying nature of optimization and the common approaches to optimization problems, random number generation, the Monte Carlo method, and the Markov chain Monte Carlo method Metaheuristic Algorithms presents common metaheuristic algorithms in detail, including genetic algorithms, simulated annealing, ant algorithms, bee algorithms, particle swarm optimization, firefly algorithms, and harmony search Applications outlines a wide range of applications that use metaheuristic algorithms to solve challenging optimization problems with detailed implementation while also introducing various modifications used for multi-objective optimization  Throughout the book, the author presents worked-out examples and real-world applications that illustrate the modern relevance of the topic. A detailed appendix features important and popular algorithms using MATLAB\textregistered{} and Octave software packages, and a related FTP site houses MATLAB code and programs for easy implementation of the discussed techniques. In addition, references to the current literature enable readers to investigate individual algorithms and methods in greater detail. Engineering Optimization: An Introduction with Metaheuristic Applications is an excellent book for courses on optimization and computer simulation at the upper-undergraduate and graduate levels. It is also a valuable reference for researchers and practitioners working in the fields of mathematics, engineering, computer science, operations research, and management science who use metaheuristic algorithms to solve problems in their everyday work.},
  isbn = {978-0-470-64041-8},
  keywords = {Mathematics / Discrete Mathematics,Mathematics / General},
  language = {en}
}

@article{yao2016,
  title = {Law of Large Numbers for Uncertain Random Variables},
  author = {Yao, K. and Gao, J.},
  year = {2016},
  volume = {24},
  pages = {615--621},
  doi = {10.1109/TFUZZ.2015.2466080},
  journal = {IEEE T. Fuzzy Systems},
  number = {3}
}

@article{zilinskas2016,
  title = {Stochastic {{Global Optimization}}: {{A Review}} on the {{Occasion}} of 25 {{Years}} of {{Informatica}}},
  shorttitle = {Stochastic {{Global Optimization}}},
  author = {{\v Z}ilinskas, Antanas and Zhigljavsky, Anatoly},
  year = {2016},
  month = jan,
  volume = {27},
  pages = {229--256},
  issn = {0868-4952, 1822-8844},
  doi = {10.15388/Informatica.2016.83},
  abstract = {This is a survey of the main achievements in the methodology and theory of stochastic global optimization. It comprises two complimentary directions: global random search and the methodology based on the use of stochastic models about the objective function. The main attention is paid to theoretically substantiated methods and mathematical results proven in the last 25 years.},
  file = {/home/akhrain/DATOS/PhD/libros_articulos/Zotero/storage/NHSUUX4H/Å½ilinskas y Zhigljavsky - 2016 - Stochastic Global Optimization A Review on the Oc.pdf},
  journal = {Informatica},
  language = {en},
  number = {2}
}


